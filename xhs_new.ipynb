{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "即将开始检查小红书登录状态...\n",
      "爬取数据有账户封禁的风险，建议使用非主账号登录。\n",
      "登录成功\n",
      "检查时间: Wed Dec  4 19:19:21 2024\n",
      "请在文本框中根据提示输入搜索关键词和笔记爬取数量。\n",
      "即将开始检查网页加载状态...\n",
      "如果网页进入人机验证页面，请先手动完成验证。\n",
      "加载成功\n",
      "检查时间: Wed Dec  4 19:19:45 2024\n",
      "已自动更改模式为图文。\n",
      "请选择排序方式:\n",
      "1. 综合\n",
      "2. 最新\n",
      "3. 最热\n",
      "请输入有效的排序方式...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "已获取的笔记数量...:  20%|██        | 1/5 [00:01<00:07,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有属性列表长度均为 5，爬取成功！\n",
      "检查时间: Wed Dec  4 19:19:48 2024\n",
      "以下为清洁数据示例:\n",
      "\n",
      "author_name: 易度泽泽-留学咨询\n",
      "like_nr: 1\n",
      "url: /66e556d9000000001e01a021?xsec_token=ABUngLbv791gP_tYthlYIVXSJB1Z1VMEPsXCdgCzrVtsI=&xsec_source=\n",
      "user_url: 66e556d9000000001e01a021?xsec_token=ABUngLbv791gP_tYthlYIVXSJB1Z1VMEPsXCdgCzrVtsI=&xsec_source=\n",
      "------\n",
      "author_name: NYUSH-NYUStern商科硕士\n",
      "like_nr: 27\n",
      "url: /6748403d000000000202cd3a?xsec_token=ABm6Qf522fzI2bYexD9bSFL-Eqt321sTe_PxIhosUXiCI=&xsec_source=\n",
      "user_url: 6748403d000000000202cd3a?xsec_token=ABm6Qf522fzI2bYexD9bSFL-Eqt321sTe_PxIhosUXiCI=&xsec_source=\n",
      "------\n",
      "author_name: 小红薯666C2C5D\n",
      "like_nr: 1\n",
      "url: /66920ed4000000002500032a?xsec_token=AB--gWjR_PTnn6MI8rkQBRlwYgv4bsk6rA7odpHQVcdJY=&xsec_source=\n",
      "user_url: 66920ed4000000002500032a?xsec_token=AB--gWjR_PTnn6MI8rkQBRlwYgv4bsk6rA7odpHQVcdJY=&xsec_source=\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "已获取的笔记数量...: 100%|██████████| 5/5 [00:24<00:00,  4.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有属性列表长度均为 5，爬取成功！\n",
      "检查时间: Wed Dec  4 19:20:13 2024\n",
      "以下为清洁数据示例:\n",
      "\n",
      "comment_nr: None\n",
      "content: 25fall的上纽申请已经开放啦！想要去上纽商学院的同学们材料准备要到尾声了，不然就很紧急了！🔥🔥🔥 今年度度依旧给大家分享了商学院四大专业的硕士班级情况！ \t 👉🏻👉🏻易度分析相比与2024届 25届班级情况的差距 作为连续3年都有上纽商科offer的申请团队，我们也做了数据的整合， 1⃣️.没有公布GRE的参考分数，但是实际情况是不少同学还是提交了的GRE的成绩的，并且基本都是320+以上的成绩 2⃣️中国本科的同学占比相比去年占比要多了，基本在30-40%，而去年美本或者海本的数量接近快90%的占比，大家都觉得真的非常难申。陆本同学的申请也非常的卷。 据我们易度今年申请提交情况，有985/211背景的居多，双非一本财经也有，但是整体录取来看情况一般。在美本背景的录取上，还是非常具有优势 3⃣️均分今年有3.5+的趋势，往年基本的录取都在3.6-3.8之间，美本3.3今年也有offer！ 4⃣️看似申请标准好像松了一些，其实并没有不卷，四个专业的申请都是非常的热门 5⃣️今年的最早申请截止日期是10月15日，所以还没有准备齐全申请材料同学加快进度！ \t 🌟🌟申请材料： 1. 申请材料清单个人信息简历全日制本科（或本科及硕士） 2.成绩单1封推荐信2篇文书（专业短文及个人描述短文） 3.先修课程GMAT或GRE（可选）*GMAT或GRE非必须要求，如选择提交分数，建议您提交线下考试中心成绩 4. TOEFL 100+；IELTS7+ 先修课程-根据各专业有单独的要求，可戳度度给您详细发送找生简章 \t ⚠️⚠️上纽大的4个商科硕士就业表现都很不错，具体可以看2023届毕业生就业报告 所以建议感兴趣的同学尽早提交申请，因为这4个项目开放申请比较早，结束申请也比较早 早申请机会往往更大一些！ \t 如果你也对商科硕士项目感兴趣，拿到名校offer ✅加入易度“菁英计划”“启明星计划”“种子计划”！ 🌟 易度也匹配了生物大方向来自康奈尔的申请导师为大家服务文书和规划 💯100+外籍文书导师文书修正，全海归毕业团队打造申请 📲全天在线接受宝子们的线上线下留学咨询 \t #留学申请季  #留学美国  #美国研究生申请  #纽约大学  #NYU  #nyu  #上海纽约大学  #上纽  #上纽大硕士  #纽大商学院  #留学机构  @校园薯\n",
      "datePublished: 2024-09-14\n",
      "images: []\n",
      "star_nr: 1\n",
      "------\n",
      "comment_nr: None\n",
      "content: 期末将至，NYU Shanghai - NYU Stern商科硕士项目的学生大使们精心推荐了适合final期间学习的地方，快一起来看看吧～ \t #上纽 #斯特恩商学院 #纽约大学 #期末 #自习 #测评\n",
      "datePublished: None\n",
      "images: []\n",
      "star_nr: 1\n",
      "------\n",
      "comment_nr: None\n",
      "content: 上纽4个热门商科硕士，2025年入学招生时间表公布！首轮申请10月截止！  各专业都安排三个轮次进行招生。其中，首轮申请均在10月份截止。由于上纽的申请比较激烈，建议同学们尽量赶在较早的申请轮次。 \t #中外合办双证硕士 #中外合作办学 #上纽 #上海纽约大学 #中外合办大学 #纽约大学 #美国留学申请\n",
      "datePublished: 2024-07-13\n",
      "images: []\n",
      "star_nr: 1\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "下载图片: 100%|██████████| 5/5 [00:00<00:00, 156503.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "删除 0 行重复行后剩余 5 行。\n",
      "检查时间: Wed Dec  4 19:20:13 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_name</th>\n",
       "      <th>comment_nr</th>\n",
       "      <th>content</th>\n",
       "      <th>datePublished</th>\n",
       "      <th>images</th>\n",
       "      <th>like_nr</th>\n",
       "      <th>star_nr</th>\n",
       "      <th>url</th>\n",
       "      <th>user_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>易度泽泽-留学咨询</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25fall的上纽申请已经开放啦！想要去上纽商学院的同学们材料准备要到尾声了，不然就很紧急了...</td>\n",
       "      <td>2024-09-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>/66e556d9000000001e01a021?xsec_token=ABUngLbv7...</td>\n",
       "      <td>66e556d9000000001e01a021?xsec_token=ABUngLbv79...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NYUSH-NYUStern商科硕士</td>\n",
       "      <td>NaN</td>\n",
       "      <td>期末将至，NYU Shanghai - NYU Stern商科硕士项目的学生大使们精心推荐了...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>/6748403d000000000202cd3a?xsec_token=ABm6Qf522...</td>\n",
       "      <td>6748403d000000000202cd3a?xsec_token=ABm6Qf522f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>小红薯666C2C5D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>上纽4个热门商科硕士，2025年入学招生时间表公布！首轮申请10月截止！  各专业都安排三个...</td>\n",
       "      <td>2024-07-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>/66920ed4000000002500032a?xsec_token=AB--gWjR_...</td>\n",
       "      <td>66920ed4000000002500032a?xsec_token=AB--gWjR_P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>留学客栈</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NYU Stern商学院 (上海纽大）offer  3️⃣周下 一年毕业🎓非常快，在上海上课...</td>\n",
       "      <td>2024-10-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>/6715c342000000001b03d845?xsec_token=ABB8nFeSq...</td>\n",
       "      <td>6715c342000000001b03d845?xsec_token=ABB8nFeSqW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>阿鱼留学咨询</td>\n",
       "      <td>11.0</td>\n",
       "      <td>上海纽约大学24fall商科硕士申请通道已经开放 💜上海纽约大学和纽约大学斯特恩（Stern...</td>\n",
       "      <td>2023-08-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>176</td>\n",
       "      <td>1</td>\n",
       "      <td>/64d39c4d000000001201fd05?xsec_token=AB8PWO8-l...</td>\n",
       "      <td>64d39c4d000000001201fd05?xsec_token=AB8PWO8-lB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author_name  comment_nr  \\\n",
       "0           易度泽泽-留学咨询         NaN   \n",
       "1  NYUSH-NYUStern商科硕士         NaN   \n",
       "2         小红薯666C2C5D         NaN   \n",
       "3                留学客栈         NaN   \n",
       "4              阿鱼留学咨询        11.0   \n",
       "\n",
       "                                             content datePublished images  \\\n",
       "0  25fall的上纽申请已经开放啦！想要去上纽商学院的同学们材料准备要到尾声了，不然就很紧急了...    2024-09-14    NaN   \n",
       "1  期末将至，NYU Shanghai - NYU Stern商科硕士项目的学生大使们精心推荐了...          None    NaN   \n",
       "2  上纽4个热门商科硕士，2025年入学招生时间表公布！首轮申请10月截止！  各专业都安排三个...    2024-07-13    NaN   \n",
       "3  NYU Stern商学院 (上海纽大）offer  3️⃣周下 一年毕业🎓非常快，在上海上课...    2024-10-20    NaN   \n",
       "4  上海纽约大学24fall商科硕士申请通道已经开放 💜上海纽约大学和纽约大学斯特恩（Stern...    2023-08-09    NaN   \n",
       "\n",
       "   like_nr  star_nr                                                url  \\\n",
       "0        1        1  /66e556d9000000001e01a021?xsec_token=ABUngLbv7...   \n",
       "1       27        1  /6748403d000000000202cd3a?xsec_token=ABm6Qf522...   \n",
       "2        1        1  /66920ed4000000002500032a?xsec_token=AB--gWjR_...   \n",
       "3        2        1  /6715c342000000001b03d845?xsec_token=ABB8nFeSq...   \n",
       "4      176        1  /64d39c4d000000001201fd05?xsec_token=AB8PWO8-l...   \n",
       "\n",
       "                                            user_url  \n",
       "0  66e556d9000000001e01a021?xsec_token=ABUngLbv79...  \n",
       "1  6748403d000000000202cd3a?xsec_token=ABm6Qf522f...  \n",
       "2  66920ed4000000002500032a?xsec_token=AB--gWjR_P...  \n",
       "3  6715c342000000001b03d845?xsec_token=ABB8nFeSqW...  \n",
       "4  64d39c4d000000001201fd05?xsec_token=AB8PWO8-lB...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已保存到 'scraped_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# 导入包(PypI)\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import requests\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "from seleniumwire import webdriver  # 使用 seleniumwire 的 webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "# 创建 'images' 文件夹，如果不存在\n",
    "images_folder = 'images'\n",
    "os.makedirs(images_folder, exist_ok=True)\n",
    "\n",
    "# 数据清洗可能需要使用的方法\n",
    "def extract_number(List):\n",
    "    list_cleaned = []\n",
    "    for x in List:\n",
    "        if x is None or not isinstance(x, str):\n",
    "            list_cleaned.append(None)\n",
    "            continue\n",
    "\n",
    "        # Find all numeric patterns in the string\n",
    "        numbers = re.findall(r'\\d+\\.?\\d*', x)\n",
    "        if numbers:\n",
    "            # Join all found numbers (if multiple numbers are in one string)\n",
    "            number = ''.join(numbers)\n",
    "            list_cleaned.append(number)\n",
    "        else:\n",
    "            list_cleaned.append(None)\n",
    "\n",
    "    return list_cleaned\n",
    "\n",
    "def extract_large_number(List):\n",
    "    list_cleaned = []\n",
    "    for x in List:\n",
    "        if x is None or not isinstance(x, str):\n",
    "            list_cleaned.append(None)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if '万' in x:\n",
    "                number = x.replace('万', '')\n",
    "                number = float(number) * 10000\n",
    "                number = int(number)\n",
    "            else:\n",
    "                number = int(float(x))\n",
    "            list_cleaned.append(number)\n",
    "        except ValueError:\n",
    "            # If conversion fails, append None or handle accordingly\n",
    "            list_cleaned.append(None)\n",
    "\n",
    "    return list_cleaned\n",
    "\n",
    "def extract_date(List):\n",
    "    list_cleaned = []\n",
    "    for x in List:\n",
    "        date_cleand = None  # Initialize with None\n",
    "        if x is None or not isinstance(x, str):\n",
    "            list_cleaned.append(date_cleand)\n",
    "            continue\n",
    "\n",
    "        match = re.search(r'(\\d{4})-(\\d{2})-(\\d{2})', x)\n",
    "        if match:\n",
    "            date_cleand = match.group(0)\n",
    "        else:\n",
    "            match = re.search(r'(\\d{2})-(\\d{2})', x)\n",
    "            if match:\n",
    "                current_year = datetime.now().year\n",
    "                month, day = match.groups()\n",
    "                date_cleand = f'{current_year}-{month}-{day}'\n",
    "\n",
    "        list_cleaned.append(date_cleand)\n",
    "\n",
    "    return list_cleaned\n",
    "\n",
    "# 配置Chrome浏览器\n",
    "service = Service(\"/opt/homebrew/bin/chromedriver\")  # 请确保 chromedriver 路径正确\n",
    "\n",
    "options = Options()\n",
    "options.add_experimental_option('debuggerAddress', '127.0.0.1:9222')  # 远程调控模式启用\n",
    "options.add_argument('--incognito')  # 隐身/无痕模式启用\n",
    "\n",
    "# 初始化 Selenium Wire WebDriver\n",
    "browser = webdriver.Chrome(service=service, options=options)\n",
    "action = ActionChains(browser)\n",
    "\n",
    "key_word = \"\"\n",
    "num = 0\n",
    "\n",
    "def check_login_status(browser):\n",
    "    print(\"即将开始检查小红书登录状态...\")\n",
    "    print(\"爬取数据有账户封禁的风险，建议使用非主账号登录。\")\n",
    "    \n",
    "    while True:\n",
    "        page_source = browser.page_source\n",
    "        if '登录探索更多内容' in page_source:\n",
    "            print('暂未登录，请手动登录')\n",
    "            print('检查时间:', time.ctime())\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            print('登录成功')\n",
    "            print('检查时间:', time.ctime())\n",
    "            time.sleep(3)\n",
    "            break\n",
    "\n",
    "def check_page_load_status(browser, keyword):\n",
    "    print(\"即将开始检查网页加载状态...\")\n",
    "    print(\"如果网页进入人机验证页面，请先手动完成验证。\")\n",
    "    \n",
    "    while True:\n",
    "        if keyword in browser.title:\n",
    "            print('加载成功')\n",
    "            print('检查时间:', time.ctime())\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(2)\n",
    "\n",
    "def selenium_test():\n",
    "    \"\"\"\n",
    "    登录状态检查，网页加载检查，根据用户输入进行搜索\n",
    "    \"\"\"\n",
    "    global key_word, num\n",
    "    browser.get('https://www.xiaohongshu.com/explore')\n",
    "    \n",
    "    check_login_status(browser)\n",
    "    \n",
    "    print(\"请在文本框中根据提示输入搜索关键词和笔记爬取数量。\")\n",
    "    keyword = input(\"搜索关键词：\")\n",
    "    try:\n",
    "        num = int(input(\"笔记爬取数量：\"))\n",
    "    except ValueError:\n",
    "        print(\"请输入有效的整数作为爬取数量。\")\n",
    "        return\n",
    "    \n",
    "    url = f'https://www.xiaohongshu.com/search_result?keyword={keyword}&source=web_explore_feed'\n",
    "    browser.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    check_page_load_status(browser, keyword)\n",
    "    \n",
    "selenium_test()\n",
    "\n",
    "def change_mode(browser):\n",
    "    # 更改模式为图文\n",
    "    try:\n",
    "        mode_button = browser.find_element(By.XPATH, '//*[@id=\"search-type\"]/div/div/div[2]')\n",
    "        mode_button.click()\n",
    "        print('已自动更改模式为图文。')\n",
    "    except Exception as e:\n",
    "        print(f\"更改模式失败: {e}\")\n",
    "\n",
    "selected_order_text = ''\n",
    "def change_sort_order(browser, action):\n",
    "    # 更改排序方式\n",
    "    sort_order = {\n",
    "        '综合': 1,\n",
    "        '最新': 2,\n",
    "        '最热': 3\n",
    "    }\n",
    "    print(\"请选择排序方式:\")\n",
    "    for order, idx in sort_order.items():\n",
    "        print(f'{idx}. {order}')\n",
    "    \n",
    "    try:\n",
    "        global selected_order_text\n",
    "        selected_order_text = input(\"请输入排序方式对应的名称: \").strip()\n",
    "        if selected_order_text not in sort_order:\n",
    "            print(\"请输入有效的排序方式...\")\n",
    "            return\n",
    "        \n",
    "        selected_order_index = sort_order[selected_order_text]\n",
    "    except Exception as e:\n",
    "        print(f\"处理排序选择时出错: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        element = browser.find_element(By.XPATH, '//*[@id=\"global\"]/div[2]/div[2]/div/div[1]/div[2]')\n",
    "        action.move_to_element(element).perform()  # 模拟鼠标悬停\n",
    "        menu = browser.find_element(By.CLASS_NAME, 'dropdown-items')\n",
    "        option = menu.find_element(By.XPATH, f'/html/body/div[4]/div/li[{selected_order_index}]')\n",
    "        option.click()  # 模拟鼠标点击\n",
    "\n",
    "        print('已选择排序方式为:', selected_order_text)\n",
    "        print('检查时间:', time.ctime())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"更改排序方式失败: {e}\")\n",
    "\n",
    "change_mode(browser)\n",
    "change_sort_order(browser, action)\n",
    "\n",
    "def parsePage(html_content, authorName_list, likeNr_list, URL_list, userURL_list, num):\n",
    "    \"\"\"\n",
    "    解析网页内容并更新数据列表。\n",
    "\n",
    "    Args:\n",
    "        html_content (str): 当前页面的HTML内容\n",
    "        authorName_list (list): 存储作者名字的列表\n",
    "        likeNr_list (list): 存储获赞数量的列表\n",
    "        URL_list (list): 存储笔记URL的列表\n",
    "        userURL_list (list): 存储用户URL的列表\n",
    "        num (int): 需要爬取的笔记数量\n",
    "\n",
    "    Returns:\n",
    "        None: 数据存储在传入的列表中\n",
    "    \"\"\"\n",
    "    response = Selector(text=html_content)\n",
    "    divs = response.xpath('//div[contains(@class, \"feeds-container\")]/section/div')  # 选中网页中包含笔记信息的部分\n",
    "\n",
    "    # 遍历divs获取每一篇笔记的信息\n",
    "    for div in divs:\n",
    "        if len(URL_list) >= num:\n",
    "            break\n",
    "        \n",
    "        if div.xpath('.//span[contains(text(), \"大家都在搜\")]'):\n",
    "            continue\n",
    "\n",
    "        # 选择并提取网页数据\n",
    "        try:\n",
    "            author_name = div.xpath('.//a[contains(@class, \"author\")]/span[contains(@class, \"name\")]/text()').get()  # 作者名字\n",
    "            like_nr = div.xpath('.//span[contains(@class, \"count\")]/text()').get()  # 获赞数量\n",
    "            url = div.xpath('.//a[contains(@class, \"cover\")]/@href').get()  # 笔记URL\n",
    "            user_url = div.xpath('.//a[contains(@class, \"author\")]/@href').get()  # 用户URL\n",
    "            \n",
    "            authorName_list.append(author_name)\n",
    "            likeNr_list.append(like_nr)\n",
    "            URL_list.append(url)\n",
    "            userURL_list.append(user_url)\n",
    "\n",
    "            time.sleep(0.35)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"解析笔记时出错: {e}\")\n",
    "            pass\n",
    "    \n",
    "    return True\n",
    "\n",
    "authorName_list, likeNr_list, URL_list, userURL_list = [], [], [], []\n",
    "qbar = tqdm(total=num, desc=\"已获取的笔记数量...\")\n",
    "\n",
    "# 检查是否已经爬取足够数量的笔记，或是否已经达到页面底部\n",
    "while len(URL_list) < num:\n",
    "    if '- THE END -' in browser.page_source:\n",
    "        print(f\"当前与{key_word}有关的笔记数量少于 {num}\")\n",
    "        print('检查时间:', time.ctime())\n",
    "        break\n",
    "    \n",
    "    parsePage(browser.page_source, authorName_list, likeNr_list, URL_list, userURL_list, num)\n",
    "    qbar.update(1)\n",
    "\n",
    "    if len(URL_list) < num:\n",
    "        browser.execute_script('window.scrollTo(0,document.body.scrollHeight)')  # 模拟鼠标滚动\n",
    "        time.sleep(random.uniform(3, 5))\n",
    "\n",
    "if len(URL_list) > num:\n",
    "    URL_list = URL_list[:num]\n",
    "    authorName_list = authorName_list[:num]\n",
    "    likeNr_list = likeNr_list[:num]\n",
    "    userURL_list = userURL_list[:num]\n",
    "\n",
    "qbar.close()\n",
    "\n",
    "if all(len(lst) == num for lst in [authorName_list, likeNr_list, URL_list, userURL_list]):\n",
    "    print(f\"所有属性列表长度均为 {num}，爬取成功！\")\n",
    "    print(f'检查时间:', time.ctime())\n",
    "else:\n",
    "    min_length = min(map(len, [authorName_list, likeNr_list, URL_list, userURL_list]))\n",
    "    print(f\"当前属性列表长度最小值为 {min_length}，请重新运行上一代码单元，直至所有属性列表长度均为 {num}！\")\n",
    "    print(f'检查时间:', time.ctime())\n",
    "\n",
    "# 清洗数据\n",
    "likeNr_list = extract_large_number(likeNr_list)\n",
    "URL_list = [re.sub(r'^/search_result/', '/', url) if url is not None else '' for url in URL_list]\n",
    "userURL_list = [url.split('/')[-1] if url is not None else '' for url in URL_list]\n",
    "\n",
    "print(\"以下为清洁数据示例:\\n\")\n",
    "for i in range(min(3, len(authorName_list))):\n",
    "    print(\"author_name:\", authorName_list[i])\n",
    "    print(\"like_nr:\", likeNr_list[i])\n",
    "    print(\"url:\", URL_list[i])\n",
    "    print(\"user_url:\", userURL_list[i])\n",
    "    print(\"------\")\n",
    "\n",
    "def parse_note_page(browser, url, commentNr_list, content_list, datePublished_list, images_list, starNr_list):\n",
    "    \"\"\"\n",
    "    解析单个笔记页面并提取所需数据\n",
    "    \n",
    "    Args:\n",
    "        browser: Selenium WebDriver 实例，用于获取页面内容\n",
    "        url: 笔记的URL\n",
    "        commentNr_list (list): 存储评论数量的列表\n",
    "        content_list (list): 存储笔记内容的列表\n",
    "        datePublished_list (list): 存储发布时间的列表\n",
    "        images_list (list of list): 存储图片链接的嵌套列表\n",
    "        starNr_list (list): 存储收藏数量的列表\n",
    "\n",
    "    Returns:\n",
    "        None: 将提取的数据添加到相应的列表中\n",
    "    \"\"\"\n",
    "    whole_url = 'https://www.xiaohongshu.com/explore' + url  # 构造完整笔记URL\n",
    "    browser.get(whole_url)\n",
    "    WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@name=\"description\"]')))  # 等待页面加载完成\n",
    "    html = browser.page_source\n",
    "    selector = Selector(text=html)\n",
    "    \n",
    "    try:\n",
    "        # 选择并提取网页数据\n",
    "        comment_nr = selector.xpath('//*[@class=\"total\"]/text()').get()  # 评论数量\n",
    "        content = selector.xpath('//*[@name=\"description\"]/@content').get()  # 内容\n",
    "        datePublished = selector.xpath('//*[@class=\"date\"]/text()').get()  # 发布时间\n",
    "        \n",
    "        # 提取图像链接（主图和头像）\n",
    "        main_image = selector.xpath('.//a[contains(@class, \"cover\")]/img/@src').get()\n",
    "        avatar_image = selector.xpath('.//a[contains(@class, \"author\")]/img/@src').get()\n",
    "        images = [main_image, avatar_image] if main_image and avatar_image else []\n",
    "        \n",
    "        star_nr = selector.xpath('//*[@class=\"count\"]/text()').get()  # 收藏数量\n",
    "\n",
    "        commentNr_list.append(comment_nr)\n",
    "        content_list.append(content)\n",
    "        datePublished_list.append(datePublished)\n",
    "        images_list.append(images)\n",
    "        starNr_list.append(star_nr)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"解析笔记页面时出错: {e}\")\n",
    "        commentNr_list.append(None)\n",
    "        content_list.append(None)\n",
    "        datePublished_list.append(None)\n",
    "        images_list.append([])\n",
    "        starNr_list.append(None)\n",
    "        \n",
    "commentNr_list, content_list, datePublished_list, images_list, starNr_list = [], [], [], [], []\n",
    "\n",
    "qbar = tqdm(total=len(URL_list), desc=\"已获取的笔记数量...\")\n",
    "for url in URL_list:\n",
    "    parse_note_page(browser, url, commentNr_list, content_list, datePublished_list, images_list, starNr_list)\n",
    "    qbar.update(1)\n",
    "    time.sleep(random.uniform(0.5, 2))\n",
    "\n",
    "qbar.close()\n",
    "\n",
    "if all(len(lst) == num for lst in [commentNr_list, content_list, datePublished_list, images_list, starNr_list]):\n",
    "    print(f\"所有属性列表长度均为 {num}，爬取成功！\")\n",
    "    print(f'检查时间:', time.ctime())\n",
    "else:\n",
    "    min_length = min(map(len, [commentNr_list, content_list, datePublished_list, images_list, starNr_list]))\n",
    "    print(f\"当前属性列表长度最小值为 {min_length}，请重新运行上一代码单元，直至所有属性列表长度均为 {num}！\")\n",
    "    print(f'检查时间:', time.ctime())\n",
    "\n",
    "# 清洗更多数据\n",
    "commentNr_list = extract_large_number(extract_number(commentNr_list))\n",
    "starNr_list = extract_large_number(starNr_list)\n",
    "datePublished_list = extract_date(datePublished_list)\n",
    "\n",
    "print(\"以下为清洁数据示例:\\n\")\n",
    "for i in range(min(3, len(commentNr_list))):\n",
    "    print(\"comment_nr:\", commentNr_list[i])\n",
    "    print(\"content:\", content_list[i])\n",
    "    print(\"datePublished:\", datePublished_list[i])\n",
    "    print(\"images:\", images_list[i])\n",
    "    print(\"star_nr:\", starNr_list[i])\n",
    "    print(\"------\")\n",
    "\n",
    "# 函数：拦截并下载图像\n",
    "def download_images_with_selenium_wire(browser, images_list, folder_path='images'):\n",
    "    \"\"\"\n",
    "    使用 Selenium Wire 拦截并下载图像。\n",
    "\n",
    "    Args:\n",
    "        browser: Selenium Wire WebDriver 实例。\n",
    "        images_list (list of list): 包含图像URL的嵌套列表。\n",
    "        folder_path (str): 图像保存的目标文件夹路径。\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # 将所有图像URL扁平化为一个列表\n",
    "    flat_images = [img_url for sublist in images_list for img_url in sublist if img_url]\n",
    "\n",
    "    # 使用集合去重，避免重复下载同一张图像\n",
    "    unique_images = list(set(flat_images))\n",
    "\n",
    "    print(f\"共找到 {len(unique_images)} 张唯一图像需要下载。\")\n",
    "\n",
    "    for img_url in tqdm(unique_images, desc=\"下载图像\"):\n",
    "        try:\n",
    "            # 清除之前的请求记录\n",
    "            browser.scopes = ['.*']  # 只拦截所有请求\n",
    "            browser.requests.clear()\n",
    "\n",
    "            # 导航到图像URL\n",
    "            browser.get(img_url)\n",
    "\n",
    "            # 等待图像响应\n",
    "            for request in browser.requests:\n",
    "                if request.response and request.url == img_url and 'image' in request.response.headers.get('Content-Type', ''):\n",
    "                    image_data = request.response.body\n",
    "\n",
    "                    # 确定图片的扩展名\n",
    "                    img_extension = os.path.splitext(img_url.split('?')[0])[1]\n",
    "                    if img_extension.lower() not in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']:\n",
    "                        img_extension = '.jpg'  # 默认扩展名\n",
    "\n",
    "                    # 构造图片文件名，避免重复\n",
    "                    img_filename = f\"{hash(img_url)}{img_extension}\"\n",
    "                    img_path = os.path.join(folder_path, img_filename)\n",
    "\n",
    "                    # 保存图片到本地\n",
    "                    with open(img_path, 'wb') as f:\n",
    "                        f.write(image_data)\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"下载图像 {img_url} 时出错：{e}\")\n",
    "\n",
    "# 下载图像\n",
    "download_images_with_selenium_wire(browser, images_list, images_folder)\n",
    "\n",
    "# 构建数据字典\n",
    "dic = {\n",
    "    # \"author_collect_nr\": authorCollectNr_list,# 作者获赞与收藏数量\n",
    "    # \"author_fans_nr\": authorFansNr_list,# 粉丝数量\n",
    "    \"author_name\": authorName_list,  # 作者名字\n",
    "    # \"author_note_nr\": authorNoteNr_list,# 作者笔记数量\n",
    "    \"comment_nr\": commentNr_list,  # 笔记评论数量\n",
    "    \"content\": content_list,  # 笔记内容\n",
    "    \"datePublished\": datePublished_list,  # 笔记发布日期\n",
    "    \"images\": images_list,  # 笔记封面图片（嵌套列表）\n",
    "    \"like_nr\": likeNr_list,  # 笔记获赞数量\n",
    "    \"star_nr\": starNr_list,\n",
    "    \"url\": URL_list,  # 笔记URL\n",
    "    \"user_url\": userURL_list  # 作者URL\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(dic)\n",
    "df = df.explode('images')  # 展开嵌套的图像列表\n",
    "df = df[~df.duplicated(keep='first')]  # 检索并删除所有属性值都相同的行,即保留第一次出现的行，删除后续的重复行\n",
    "print(\"删除\", num - len(df), \"行重复行后剩余\", len(df), \"行。\")\n",
    "print('检查时间:', time.ctime())\n",
    "display(df.head(10))\n",
    "df.to_csv('scraped_data.csv', encoding='utf-8-sig')\n",
    "print(\"数据已保存到 'scraped_data.csv'\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xiaoliu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
