{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å³å°†å¼€å§‹æ£€æŸ¥å°çº¢ä¹¦ç™»å½•çŠ¶æ€...\n",
      "çˆ¬å–æ•°æ®æœ‰è´¦æˆ·å°ç¦çš„é£é™©ï¼Œå»ºè®®ä½¿ç”¨éä¸»è´¦å·ç™»å½•ã€‚\n",
      "ç™»å½•æˆåŠŸ\n",
      "æ£€æŸ¥æ—¶é—´: Wed Dec  4 19:19:21 2024\n",
      "è¯·åœ¨æ–‡æœ¬æ¡†ä¸­æ ¹æ®æç¤ºè¾“å…¥æœç´¢å…³é”®è¯å’Œç¬”è®°çˆ¬å–æ•°é‡ã€‚\n",
      "å³å°†å¼€å§‹æ£€æŸ¥ç½‘é¡µåŠ è½½çŠ¶æ€...\n",
      "å¦‚æœç½‘é¡µè¿›å…¥äººæœºéªŒè¯é¡µé¢ï¼Œè¯·å…ˆæ‰‹åŠ¨å®ŒæˆéªŒè¯ã€‚\n",
      "åŠ è½½æˆåŠŸ\n",
      "æ£€æŸ¥æ—¶é—´: Wed Dec  4 19:19:45 2024\n",
      "å·²è‡ªåŠ¨æ›´æ”¹æ¨¡å¼ä¸ºå›¾æ–‡ã€‚\n",
      "è¯·é€‰æ‹©æ’åºæ–¹å¼:\n",
      "1. ç»¼åˆ\n",
      "2. æœ€æ–°\n",
      "3. æœ€çƒ­\n",
      "è¯·è¾“å…¥æœ‰æ•ˆçš„æ’åºæ–¹å¼...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å·²è·å–çš„ç¬”è®°æ•°é‡...:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:07,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰€æœ‰å±æ€§åˆ—è¡¨é•¿åº¦å‡ä¸º 5ï¼Œçˆ¬å–æˆåŠŸï¼\n",
      "æ£€æŸ¥æ—¶é—´: Wed Dec  4 19:19:48 2024\n",
      "ä»¥ä¸‹ä¸ºæ¸…æ´æ•°æ®ç¤ºä¾‹:\n",
      "\n",
      "author_name: æ˜“åº¦æ³½æ³½-ç•™å­¦å’¨è¯¢\n",
      "like_nr: 1\n",
      "url: /66e556d9000000001e01a021?xsec_token=ABUngLbv791gP_tYthlYIVXSJB1Z1VMEPsXCdgCzrVtsI=&xsec_source=\n",
      "user_url: 66e556d9000000001e01a021?xsec_token=ABUngLbv791gP_tYthlYIVXSJB1Z1VMEPsXCdgCzrVtsI=&xsec_source=\n",
      "------\n",
      "author_name: NYUSH-NYUSternå•†ç§‘ç¡•å£«\n",
      "like_nr: 27\n",
      "url: /6748403d000000000202cd3a?xsec_token=ABm6Qf522fzI2bYexD9bSFL-Eqt321sTe_PxIhosUXiCI=&xsec_source=\n",
      "user_url: 6748403d000000000202cd3a?xsec_token=ABm6Qf522fzI2bYexD9bSFL-Eqt321sTe_PxIhosUXiCI=&xsec_source=\n",
      "------\n",
      "author_name: å°çº¢è–¯666C2C5D\n",
      "like_nr: 1\n",
      "url: /66920ed4000000002500032a?xsec_token=AB--gWjR_PTnn6MI8rkQBRlwYgv4bsk6rA7odpHQVcdJY=&xsec_source=\n",
      "user_url: 66920ed4000000002500032a?xsec_token=AB--gWjR_PTnn6MI8rkQBRlwYgv4bsk6rA7odpHQVcdJY=&xsec_source=\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å·²è·å–çš„ç¬”è®°æ•°é‡...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:24<00:00,  4.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰€æœ‰å±æ€§åˆ—è¡¨é•¿åº¦å‡ä¸º 5ï¼Œçˆ¬å–æˆåŠŸï¼\n",
      "æ£€æŸ¥æ—¶é—´: Wed Dec  4 19:20:13 2024\n",
      "ä»¥ä¸‹ä¸ºæ¸…æ´æ•°æ®ç¤ºä¾‹:\n",
      "\n",
      "comment_nr: None\n",
      "content: 25fallçš„ä¸Šçº½ç”³è¯·å·²ç»å¼€æ”¾å•¦ï¼æƒ³è¦å»ä¸Šçº½å•†å­¦é™¢çš„åŒå­¦ä»¬ææ–™å‡†å¤‡è¦åˆ°å°¾å£°äº†ï¼Œä¸ç„¶å°±å¾ˆç´§æ€¥äº†ï¼ğŸ”¥ğŸ”¥ğŸ”¥ ä»Šå¹´åº¦åº¦ä¾æ—§ç»™å¤§å®¶åˆ†äº«äº†å•†å­¦é™¢å››å¤§ä¸“ä¸šçš„ç¡•å£«ç­çº§æƒ…å†µï¼ \t ğŸ‘‰ğŸ»ğŸ‘‰ğŸ»æ˜“åº¦åˆ†æç›¸æ¯”ä¸2024å±Š 25å±Šç­çº§æƒ…å†µçš„å·®è· ä½œä¸ºè¿ç»­3å¹´éƒ½æœ‰ä¸Šçº½å•†ç§‘offerçš„ç”³è¯·å›¢é˜Ÿï¼Œæˆ‘ä»¬ä¹Ÿåšäº†æ•°æ®çš„æ•´åˆï¼Œ 1âƒ£ï¸.æ²¡æœ‰å…¬å¸ƒGREçš„å‚è€ƒåˆ†æ•°ï¼Œä½†æ˜¯å®é™…æƒ…å†µæ˜¯ä¸å°‘åŒå­¦è¿˜æ˜¯æäº¤äº†çš„GREçš„æˆç»©çš„ï¼Œå¹¶ä¸”åŸºæœ¬éƒ½æ˜¯320+ä»¥ä¸Šçš„æˆç»© 2âƒ£ï¸ä¸­å›½æœ¬ç§‘çš„åŒå­¦å æ¯”ç›¸æ¯”å»å¹´å æ¯”è¦å¤šäº†ï¼ŒåŸºæœ¬åœ¨30-40%ï¼Œè€Œå»å¹´ç¾æœ¬æˆ–è€…æµ·æœ¬çš„æ•°é‡æ¥è¿‘å¿«90%çš„å æ¯”ï¼Œå¤§å®¶éƒ½è§‰å¾—çœŸçš„éå¸¸éš¾ç”³ã€‚é™†æœ¬åŒå­¦çš„ç”³è¯·ä¹Ÿéå¸¸çš„å·ã€‚ æ®æˆ‘ä»¬æ˜“åº¦ä»Šå¹´ç”³è¯·æäº¤æƒ…å†µï¼Œæœ‰985/211èƒŒæ™¯çš„å±…å¤šï¼ŒåŒéä¸€æœ¬è´¢ç»ä¹Ÿæœ‰ï¼Œä½†æ˜¯æ•´ä½“å½•å–æ¥çœ‹æƒ…å†µä¸€èˆ¬ã€‚åœ¨ç¾æœ¬èƒŒæ™¯çš„å½•å–ä¸Šï¼Œè¿˜æ˜¯éå¸¸å…·æœ‰ä¼˜åŠ¿ 3âƒ£ï¸å‡åˆ†ä»Šå¹´æœ‰3.5+çš„è¶‹åŠ¿ï¼Œå¾€å¹´åŸºæœ¬çš„å½•å–éƒ½åœ¨3.6-3.8ä¹‹é—´ï¼Œç¾æœ¬3.3ä»Šå¹´ä¹Ÿæœ‰offerï¼ 4âƒ£ï¸çœ‹ä¼¼ç”³è¯·æ ‡å‡†å¥½åƒæ¾äº†ä¸€äº›ï¼Œå…¶å®å¹¶æ²¡æœ‰ä¸å·ï¼Œå››ä¸ªä¸“ä¸šçš„ç”³è¯·éƒ½æ˜¯éå¸¸çš„çƒ­é—¨ 5âƒ£ï¸ä»Šå¹´çš„æœ€æ—©ç”³è¯·æˆªæ­¢æ—¥æœŸæ˜¯10æœˆ15æ—¥ï¼Œæ‰€ä»¥è¿˜æ²¡æœ‰å‡†å¤‡é½å…¨ç”³è¯·ææ–™åŒå­¦åŠ å¿«è¿›åº¦ï¼ \t ğŸŒŸğŸŒŸç”³è¯·ææ–™ï¼š 1. ç”³è¯·ææ–™æ¸…å•ä¸ªäººä¿¡æ¯ç®€å†å…¨æ—¥åˆ¶æœ¬ç§‘ï¼ˆæˆ–æœ¬ç§‘åŠç¡•å£«ï¼‰ 2.æˆç»©å•1å°æ¨èä¿¡2ç¯‡æ–‡ä¹¦ï¼ˆä¸“ä¸šçŸ­æ–‡åŠä¸ªäººæè¿°çŸ­æ–‡ï¼‰ 3.å…ˆä¿®è¯¾ç¨‹GMATæˆ–GREï¼ˆå¯é€‰ï¼‰*GMATæˆ–GREéå¿…é¡»è¦æ±‚ï¼Œå¦‚é€‰æ‹©æäº¤åˆ†æ•°ï¼Œå»ºè®®æ‚¨æäº¤çº¿ä¸‹è€ƒè¯•ä¸­å¿ƒæˆç»© 4. TOEFL 100+ï¼›IELTS7+ å…ˆä¿®è¯¾ç¨‹-æ ¹æ®å„ä¸“ä¸šæœ‰å•ç‹¬çš„è¦æ±‚ï¼Œå¯æˆ³åº¦åº¦ç»™æ‚¨è¯¦ç»†å‘é€æ‰¾ç”Ÿç®€ç«  \t âš ï¸âš ï¸ä¸Šçº½å¤§çš„4ä¸ªå•†ç§‘ç¡•å£«å°±ä¸šè¡¨ç°éƒ½å¾ˆä¸é”™ï¼Œå…·ä½“å¯ä»¥çœ‹2023å±Šæ¯•ä¸šç”Ÿå°±ä¸šæŠ¥å‘Š æ‰€ä»¥å»ºè®®æ„Ÿå…´è¶£çš„åŒå­¦å°½æ—©æäº¤ç”³è¯·ï¼Œå› ä¸ºè¿™4ä¸ªé¡¹ç›®å¼€æ”¾ç”³è¯·æ¯”è¾ƒæ—©ï¼Œç»“æŸç”³è¯·ä¹Ÿæ¯”è¾ƒæ—© æ—©ç”³è¯·æœºä¼šå¾€å¾€æ›´å¤§ä¸€äº›ï¼ \t å¦‚æœä½ ä¹Ÿå¯¹å•†ç§‘ç¡•å£«é¡¹ç›®æ„Ÿå…´è¶£ï¼Œæ‹¿åˆ°åæ ¡offer âœ…åŠ å…¥æ˜“åº¦â€œèè‹±è®¡åˆ’â€â€œå¯æ˜æ˜Ÿè®¡åˆ’â€â€œç§å­è®¡åˆ’â€ï¼ ğŸŒŸ æ˜“åº¦ä¹ŸåŒ¹é…äº†ç”Ÿç‰©å¤§æ–¹å‘æ¥è‡ªåº·å¥ˆå°”çš„ç”³è¯·å¯¼å¸ˆä¸ºå¤§å®¶æœåŠ¡æ–‡ä¹¦å’Œè§„åˆ’ ğŸ’¯100+å¤–ç±æ–‡ä¹¦å¯¼å¸ˆæ–‡ä¹¦ä¿®æ­£ï¼Œå…¨æµ·å½’æ¯•ä¸šå›¢é˜Ÿæ‰“é€ ç”³è¯· ğŸ“²å…¨å¤©åœ¨çº¿æ¥å—å®å­ä»¬çš„çº¿ä¸Šçº¿ä¸‹ç•™å­¦å’¨è¯¢ \t #ç•™å­¦ç”³è¯·å­£Â Â #ç•™å­¦ç¾å›½Â Â #ç¾å›½ç ”ç©¶ç”Ÿç”³è¯·Â Â #çº½çº¦å¤§å­¦Â Â #NYUÂ Â #nyuÂ Â #ä¸Šæµ·çº½çº¦å¤§å­¦Â Â #ä¸Šçº½Â Â #ä¸Šçº½å¤§ç¡•å£«Â Â #çº½å¤§å•†å­¦é™¢Â Â #ç•™å­¦æœºæ„Â Â @æ ¡å›­è–¯\n",
      "datePublished: 2024-09-14\n",
      "images: []\n",
      "star_nr: 1\n",
      "------\n",
      "comment_nr: None\n",
      "content: æœŸæœ«å°†è‡³ï¼ŒNYU Shanghai - NYU Sternå•†ç§‘ç¡•å£«é¡¹ç›®çš„å­¦ç”Ÿå¤§ä½¿ä»¬ç²¾å¿ƒæ¨èäº†é€‚åˆfinalæœŸé—´å­¦ä¹ çš„åœ°æ–¹ï¼Œå¿«ä¸€èµ·æ¥çœ‹çœ‹å§ï½ \t #ä¸Šçº½ #æ–¯ç‰¹æ©å•†å­¦é™¢ #çº½çº¦å¤§å­¦ #æœŸæœ« #è‡ªä¹  #æµ‹è¯„\n",
      "datePublished: None\n",
      "images: []\n",
      "star_nr: 1\n",
      "------\n",
      "comment_nr: None\n",
      "content: ä¸Šçº½4ä¸ªçƒ­é—¨å•†ç§‘ç¡•å£«ï¼Œ2025å¹´å…¥å­¦æ‹›ç”Ÿæ—¶é—´è¡¨å…¬å¸ƒï¼é¦–è½®ç”³è¯·10æœˆæˆªæ­¢ï¼  å„ä¸“ä¸šéƒ½å®‰æ’ä¸‰ä¸ªè½®æ¬¡è¿›è¡Œæ‹›ç”Ÿã€‚å…¶ä¸­ï¼Œé¦–è½®ç”³è¯·å‡åœ¨10æœˆä»½æˆªæ­¢ã€‚ç”±äºä¸Šçº½çš„ç”³è¯·æ¯”è¾ƒæ¿€çƒˆï¼Œå»ºè®®åŒå­¦ä»¬å°½é‡èµ¶åœ¨è¾ƒæ—©çš„ç”³è¯·è½®æ¬¡ã€‚ \t #ä¸­å¤–åˆåŠåŒè¯ç¡•å£« #ä¸­å¤–åˆä½œåŠå­¦ #ä¸Šçº½ #ä¸Šæµ·çº½çº¦å¤§å­¦ #ä¸­å¤–åˆåŠå¤§å­¦ #çº½çº¦å¤§å­¦ #ç¾å›½ç•™å­¦ç”³è¯·\n",
      "datePublished: 2024-07-13\n",
      "images: []\n",
      "star_nr: 1\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ä¸‹è½½å›¾ç‰‡: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 156503.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆ é™¤ 0 è¡Œé‡å¤è¡Œåå‰©ä½™ 5 è¡Œã€‚\n",
      "æ£€æŸ¥æ—¶é—´: Wed Dec  4 19:20:13 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_name</th>\n",
       "      <th>comment_nr</th>\n",
       "      <th>content</th>\n",
       "      <th>datePublished</th>\n",
       "      <th>images</th>\n",
       "      <th>like_nr</th>\n",
       "      <th>star_nr</th>\n",
       "      <th>url</th>\n",
       "      <th>user_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>æ˜“åº¦æ³½æ³½-ç•™å­¦å’¨è¯¢</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25fallçš„ä¸Šçº½ç”³è¯·å·²ç»å¼€æ”¾å•¦ï¼æƒ³è¦å»ä¸Šçº½å•†å­¦é™¢çš„åŒå­¦ä»¬ææ–™å‡†å¤‡è¦åˆ°å°¾å£°äº†ï¼Œä¸ç„¶å°±å¾ˆç´§æ€¥äº†...</td>\n",
       "      <td>2024-09-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>/66e556d9000000001e01a021?xsec_token=ABUngLbv7...</td>\n",
       "      <td>66e556d9000000001e01a021?xsec_token=ABUngLbv79...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NYUSH-NYUSternå•†ç§‘ç¡•å£«</td>\n",
       "      <td>NaN</td>\n",
       "      <td>æœŸæœ«å°†è‡³ï¼ŒNYU Shanghai - NYU Sternå•†ç§‘ç¡•å£«é¡¹ç›®çš„å­¦ç”Ÿå¤§ä½¿ä»¬ç²¾å¿ƒæ¨èäº†...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>/6748403d000000000202cd3a?xsec_token=ABm6Qf522...</td>\n",
       "      <td>6748403d000000000202cd3a?xsec_token=ABm6Qf522f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>å°çº¢è–¯666C2C5D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ä¸Šçº½4ä¸ªçƒ­é—¨å•†ç§‘ç¡•å£«ï¼Œ2025å¹´å…¥å­¦æ‹›ç”Ÿæ—¶é—´è¡¨å…¬å¸ƒï¼é¦–è½®ç”³è¯·10æœˆæˆªæ­¢ï¼  å„ä¸“ä¸šéƒ½å®‰æ’ä¸‰ä¸ª...</td>\n",
       "      <td>2024-07-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>/66920ed4000000002500032a?xsec_token=AB--gWjR_...</td>\n",
       "      <td>66920ed4000000002500032a?xsec_token=AB--gWjR_P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ç•™å­¦å®¢æ ˆ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NYU Sternå•†å­¦é™¢ (ä¸Šæµ·çº½å¤§ï¼‰offer  3ï¸âƒ£å‘¨ä¸‹ ä¸€å¹´æ¯•ä¸šğŸ“éå¸¸å¿«ï¼Œåœ¨ä¸Šæµ·ä¸Šè¯¾...</td>\n",
       "      <td>2024-10-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>/6715c342000000001b03d845?xsec_token=ABB8nFeSq...</td>\n",
       "      <td>6715c342000000001b03d845?xsec_token=ABB8nFeSqW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>é˜¿é±¼ç•™å­¦å’¨è¯¢</td>\n",
       "      <td>11.0</td>\n",
       "      <td>ä¸Šæµ·çº½çº¦å¤§å­¦24fallå•†ç§‘ç¡•å£«ç”³è¯·é€šé“å·²ç»å¼€æ”¾ ğŸ’œä¸Šæµ·çº½çº¦å¤§å­¦å’Œçº½çº¦å¤§å­¦æ–¯ç‰¹æ©ï¼ˆStern...</td>\n",
       "      <td>2023-08-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>176</td>\n",
       "      <td>1</td>\n",
       "      <td>/64d39c4d000000001201fd05?xsec_token=AB8PWO8-l...</td>\n",
       "      <td>64d39c4d000000001201fd05?xsec_token=AB8PWO8-lB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author_name  comment_nr  \\\n",
       "0           æ˜“åº¦æ³½æ³½-ç•™å­¦å’¨è¯¢         NaN   \n",
       "1  NYUSH-NYUSternå•†ç§‘ç¡•å£«         NaN   \n",
       "2         å°çº¢è–¯666C2C5D         NaN   \n",
       "3                ç•™å­¦å®¢æ ˆ         NaN   \n",
       "4              é˜¿é±¼ç•™å­¦å’¨è¯¢        11.0   \n",
       "\n",
       "                                             content datePublished images  \\\n",
       "0  25fallçš„ä¸Šçº½ç”³è¯·å·²ç»å¼€æ”¾å•¦ï¼æƒ³è¦å»ä¸Šçº½å•†å­¦é™¢çš„åŒå­¦ä»¬ææ–™å‡†å¤‡è¦åˆ°å°¾å£°äº†ï¼Œä¸ç„¶å°±å¾ˆç´§æ€¥äº†...    2024-09-14    NaN   \n",
       "1  æœŸæœ«å°†è‡³ï¼ŒNYU Shanghai - NYU Sternå•†ç§‘ç¡•å£«é¡¹ç›®çš„å­¦ç”Ÿå¤§ä½¿ä»¬ç²¾å¿ƒæ¨èäº†...          None    NaN   \n",
       "2  ä¸Šçº½4ä¸ªçƒ­é—¨å•†ç§‘ç¡•å£«ï¼Œ2025å¹´å…¥å­¦æ‹›ç”Ÿæ—¶é—´è¡¨å…¬å¸ƒï¼é¦–è½®ç”³è¯·10æœˆæˆªæ­¢ï¼  å„ä¸“ä¸šéƒ½å®‰æ’ä¸‰ä¸ª...    2024-07-13    NaN   \n",
       "3  NYU Sternå•†å­¦é™¢ (ä¸Šæµ·çº½å¤§ï¼‰offer  3ï¸âƒ£å‘¨ä¸‹ ä¸€å¹´æ¯•ä¸šğŸ“éå¸¸å¿«ï¼Œåœ¨ä¸Šæµ·ä¸Šè¯¾...    2024-10-20    NaN   \n",
       "4  ä¸Šæµ·çº½çº¦å¤§å­¦24fallå•†ç§‘ç¡•å£«ç”³è¯·é€šé“å·²ç»å¼€æ”¾ ğŸ’œä¸Šæµ·çº½çº¦å¤§å­¦å’Œçº½çº¦å¤§å­¦æ–¯ç‰¹æ©ï¼ˆStern...    2023-08-09    NaN   \n",
       "\n",
       "   like_nr  star_nr                                                url  \\\n",
       "0        1        1  /66e556d9000000001e01a021?xsec_token=ABUngLbv7...   \n",
       "1       27        1  /6748403d000000000202cd3a?xsec_token=ABm6Qf522...   \n",
       "2        1        1  /66920ed4000000002500032a?xsec_token=AB--gWjR_...   \n",
       "3        2        1  /6715c342000000001b03d845?xsec_token=ABB8nFeSq...   \n",
       "4      176        1  /64d39c4d000000001201fd05?xsec_token=AB8PWO8-l...   \n",
       "\n",
       "                                            user_url  \n",
       "0  66e556d9000000001e01a021?xsec_token=ABUngLbv79...  \n",
       "1  6748403d000000000202cd3a?xsec_token=ABm6Qf522f...  \n",
       "2  66920ed4000000002500032a?xsec_token=AB--gWjR_P...  \n",
       "3  6715c342000000001b03d845?xsec_token=ABB8nFeSqW...  \n",
       "4  64d39c4d000000001201fd05?xsec_token=AB8PWO8-lB...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®å·²ä¿å­˜åˆ° 'scraped_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# å¯¼å…¥åŒ…(PypI)\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import requests\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "from seleniumwire import webdriver  # ä½¿ç”¨ seleniumwire çš„ webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "# åˆ›å»º 'images' æ–‡ä»¶å¤¹ï¼Œå¦‚æœä¸å­˜åœ¨\n",
    "images_folder = 'images'\n",
    "os.makedirs(images_folder, exist_ok=True)\n",
    "\n",
    "# æ•°æ®æ¸…æ´—å¯èƒ½éœ€è¦ä½¿ç”¨çš„æ–¹æ³•\n",
    "def extract_number(List):\n",
    "    list_cleaned = []\n",
    "    for x in List:\n",
    "        if x is None or not isinstance(x, str):\n",
    "            list_cleaned.append(None)\n",
    "            continue\n",
    "\n",
    "        # Find all numeric patterns in the string\n",
    "        numbers = re.findall(r'\\d+\\.?\\d*', x)\n",
    "        if numbers:\n",
    "            # Join all found numbers (if multiple numbers are in one string)\n",
    "            number = ''.join(numbers)\n",
    "            list_cleaned.append(number)\n",
    "        else:\n",
    "            list_cleaned.append(None)\n",
    "\n",
    "    return list_cleaned\n",
    "\n",
    "def extract_large_number(List):\n",
    "    list_cleaned = []\n",
    "    for x in List:\n",
    "        if x is None or not isinstance(x, str):\n",
    "            list_cleaned.append(None)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if 'ä¸‡' in x:\n",
    "                number = x.replace('ä¸‡', '')\n",
    "                number = float(number) * 10000\n",
    "                number = int(number)\n",
    "            else:\n",
    "                number = int(float(x))\n",
    "            list_cleaned.append(number)\n",
    "        except ValueError:\n",
    "            # If conversion fails, append None or handle accordingly\n",
    "            list_cleaned.append(None)\n",
    "\n",
    "    return list_cleaned\n",
    "\n",
    "def extract_date(List):\n",
    "    list_cleaned = []\n",
    "    for x in List:\n",
    "        date_cleand = None  # Initialize with None\n",
    "        if x is None or not isinstance(x, str):\n",
    "            list_cleaned.append(date_cleand)\n",
    "            continue\n",
    "\n",
    "        match = re.search(r'(\\d{4})-(\\d{2})-(\\d{2})', x)\n",
    "        if match:\n",
    "            date_cleand = match.group(0)\n",
    "        else:\n",
    "            match = re.search(r'(\\d{2})-(\\d{2})', x)\n",
    "            if match:\n",
    "                current_year = datetime.now().year\n",
    "                month, day = match.groups()\n",
    "                date_cleand = f'{current_year}-{month}-{day}'\n",
    "\n",
    "        list_cleaned.append(date_cleand)\n",
    "\n",
    "    return list_cleaned\n",
    "\n",
    "# é…ç½®Chromeæµè§ˆå™¨\n",
    "service = Service(\"/opt/homebrew/bin/chromedriver\")  # è¯·ç¡®ä¿ chromedriver è·¯å¾„æ­£ç¡®\n",
    "\n",
    "options = Options()\n",
    "options.add_experimental_option('debuggerAddress', '127.0.0.1:9222')  # è¿œç¨‹è°ƒæ§æ¨¡å¼å¯ç”¨\n",
    "options.add_argument('--incognito')  # éšèº«/æ— ç—•æ¨¡å¼å¯ç”¨\n",
    "\n",
    "# åˆå§‹åŒ– Selenium Wire WebDriver\n",
    "browser = webdriver.Chrome(service=service, options=options)\n",
    "action = ActionChains(browser)\n",
    "\n",
    "key_word = \"\"\n",
    "num = 0\n",
    "\n",
    "def check_login_status(browser):\n",
    "    print(\"å³å°†å¼€å§‹æ£€æŸ¥å°çº¢ä¹¦ç™»å½•çŠ¶æ€...\")\n",
    "    print(\"çˆ¬å–æ•°æ®æœ‰è´¦æˆ·å°ç¦çš„é£é™©ï¼Œå»ºè®®ä½¿ç”¨éä¸»è´¦å·ç™»å½•ã€‚\")\n",
    "    \n",
    "    while True:\n",
    "        page_source = browser.page_source\n",
    "        if 'ç™»å½•æ¢ç´¢æ›´å¤šå†…å®¹' in page_source:\n",
    "            print('æš‚æœªç™»å½•ï¼Œè¯·æ‰‹åŠ¨ç™»å½•')\n",
    "            print('æ£€æŸ¥æ—¶é—´:', time.ctime())\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            print('ç™»å½•æˆåŠŸ')\n",
    "            print('æ£€æŸ¥æ—¶é—´:', time.ctime())\n",
    "            time.sleep(3)\n",
    "            break\n",
    "\n",
    "def check_page_load_status(browser, keyword):\n",
    "    print(\"å³å°†å¼€å§‹æ£€æŸ¥ç½‘é¡µåŠ è½½çŠ¶æ€...\")\n",
    "    print(\"å¦‚æœç½‘é¡µè¿›å…¥äººæœºéªŒè¯é¡µé¢ï¼Œè¯·å…ˆæ‰‹åŠ¨å®ŒæˆéªŒè¯ã€‚\")\n",
    "    \n",
    "    while True:\n",
    "        if keyword in browser.title:\n",
    "            print('åŠ è½½æˆåŠŸ')\n",
    "            print('æ£€æŸ¥æ—¶é—´:', time.ctime())\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(2)\n",
    "\n",
    "def selenium_test():\n",
    "    \"\"\"\n",
    "    ç™»å½•çŠ¶æ€æ£€æŸ¥ï¼Œç½‘é¡µåŠ è½½æ£€æŸ¥ï¼Œæ ¹æ®ç”¨æˆ·è¾“å…¥è¿›è¡Œæœç´¢\n",
    "    \"\"\"\n",
    "    global key_word, num\n",
    "    browser.get('https://www.xiaohongshu.com/explore')\n",
    "    \n",
    "    check_login_status(browser)\n",
    "    \n",
    "    print(\"è¯·åœ¨æ–‡æœ¬æ¡†ä¸­æ ¹æ®æç¤ºè¾“å…¥æœç´¢å…³é”®è¯å’Œç¬”è®°çˆ¬å–æ•°é‡ã€‚\")\n",
    "    keyword = input(\"æœç´¢å…³é”®è¯ï¼š\")\n",
    "    try:\n",
    "        num = int(input(\"ç¬”è®°çˆ¬å–æ•°é‡ï¼š\"))\n",
    "    except ValueError:\n",
    "        print(\"è¯·è¾“å…¥æœ‰æ•ˆçš„æ•´æ•°ä½œä¸ºçˆ¬å–æ•°é‡ã€‚\")\n",
    "        return\n",
    "    \n",
    "    url = f'https://www.xiaohongshu.com/search_result?keyword={keyword}&source=web_explore_feed'\n",
    "    browser.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    check_page_load_status(browser, keyword)\n",
    "    \n",
    "selenium_test()\n",
    "\n",
    "def change_mode(browser):\n",
    "    # æ›´æ”¹æ¨¡å¼ä¸ºå›¾æ–‡\n",
    "    try:\n",
    "        mode_button = browser.find_element(By.XPATH, '//*[@id=\"search-type\"]/div/div/div[2]')\n",
    "        mode_button.click()\n",
    "        print('å·²è‡ªåŠ¨æ›´æ”¹æ¨¡å¼ä¸ºå›¾æ–‡ã€‚')\n",
    "    except Exception as e:\n",
    "        print(f\"æ›´æ”¹æ¨¡å¼å¤±è´¥: {e}\")\n",
    "\n",
    "selected_order_text = ''\n",
    "def change_sort_order(browser, action):\n",
    "    # æ›´æ”¹æ’åºæ–¹å¼\n",
    "    sort_order = {\n",
    "        'ç»¼åˆ': 1,\n",
    "        'æœ€æ–°': 2,\n",
    "        'æœ€çƒ­': 3\n",
    "    }\n",
    "    print(\"è¯·é€‰æ‹©æ’åºæ–¹å¼:\")\n",
    "    for order, idx in sort_order.items():\n",
    "        print(f'{idx}. {order}')\n",
    "    \n",
    "    try:\n",
    "        global selected_order_text\n",
    "        selected_order_text = input(\"è¯·è¾“å…¥æ’åºæ–¹å¼å¯¹åº”çš„åç§°: \").strip()\n",
    "        if selected_order_text not in sort_order:\n",
    "            print(\"è¯·è¾“å…¥æœ‰æ•ˆçš„æ’åºæ–¹å¼...\")\n",
    "            return\n",
    "        \n",
    "        selected_order_index = sort_order[selected_order_text]\n",
    "    except Exception as e:\n",
    "        print(f\"å¤„ç†æ’åºé€‰æ‹©æ—¶å‡ºé”™: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        element = browser.find_element(By.XPATH, '//*[@id=\"global\"]/div[2]/div[2]/div/div[1]/div[2]')\n",
    "        action.move_to_element(element).perform()  # æ¨¡æ‹Ÿé¼ æ ‡æ‚¬åœ\n",
    "        menu = browser.find_element(By.CLASS_NAME, 'dropdown-items')\n",
    "        option = menu.find_element(By.XPATH, f'/html/body/div[4]/div/li[{selected_order_index}]')\n",
    "        option.click()  # æ¨¡æ‹Ÿé¼ æ ‡ç‚¹å‡»\n",
    "\n",
    "        print('å·²é€‰æ‹©æ’åºæ–¹å¼ä¸º:', selected_order_text)\n",
    "        print('æ£€æŸ¥æ—¶é—´:', time.ctime())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"æ›´æ”¹æ’åºæ–¹å¼å¤±è´¥: {e}\")\n",
    "\n",
    "change_mode(browser)\n",
    "change_sort_order(browser, action)\n",
    "\n",
    "def parsePage(html_content, authorName_list, likeNr_list, URL_list, userURL_list, num):\n",
    "    \"\"\"\n",
    "    è§£æç½‘é¡µå†…å®¹å¹¶æ›´æ–°æ•°æ®åˆ—è¡¨ã€‚\n",
    "\n",
    "    Args:\n",
    "        html_content (str): å½“å‰é¡µé¢çš„HTMLå†…å®¹\n",
    "        authorName_list (list): å­˜å‚¨ä½œè€…åå­—çš„åˆ—è¡¨\n",
    "        likeNr_list (list): å­˜å‚¨è·èµæ•°é‡çš„åˆ—è¡¨\n",
    "        URL_list (list): å­˜å‚¨ç¬”è®°URLçš„åˆ—è¡¨\n",
    "        userURL_list (list): å­˜å‚¨ç”¨æˆ·URLçš„åˆ—è¡¨\n",
    "        num (int): éœ€è¦çˆ¬å–çš„ç¬”è®°æ•°é‡\n",
    "\n",
    "    Returns:\n",
    "        None: æ•°æ®å­˜å‚¨åœ¨ä¼ å…¥çš„åˆ—è¡¨ä¸­\n",
    "    \"\"\"\n",
    "    response = Selector(text=html_content)\n",
    "    divs = response.xpath('//div[contains(@class, \"feeds-container\")]/section/div')  # é€‰ä¸­ç½‘é¡µä¸­åŒ…å«ç¬”è®°ä¿¡æ¯çš„éƒ¨åˆ†\n",
    "\n",
    "    # éå†divsè·å–æ¯ä¸€ç¯‡ç¬”è®°çš„ä¿¡æ¯\n",
    "    for div in divs:\n",
    "        if len(URL_list) >= num:\n",
    "            break\n",
    "        \n",
    "        if div.xpath('.//span[contains(text(), \"å¤§å®¶éƒ½åœ¨æœ\")]'):\n",
    "            continue\n",
    "\n",
    "        # é€‰æ‹©å¹¶æå–ç½‘é¡µæ•°æ®\n",
    "        try:\n",
    "            author_name = div.xpath('.//a[contains(@class, \"author\")]/span[contains(@class, \"name\")]/text()').get()  # ä½œè€…åå­—\n",
    "            like_nr = div.xpath('.//span[contains(@class, \"count\")]/text()').get()  # è·èµæ•°é‡\n",
    "            url = div.xpath('.//a[contains(@class, \"cover\")]/@href').get()  # ç¬”è®°URL\n",
    "            user_url = div.xpath('.//a[contains(@class, \"author\")]/@href').get()  # ç”¨æˆ·URL\n",
    "            \n",
    "            authorName_list.append(author_name)\n",
    "            likeNr_list.append(like_nr)\n",
    "            URL_list.append(url)\n",
    "            userURL_list.append(user_url)\n",
    "\n",
    "            time.sleep(0.35)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"è§£æç¬”è®°æ—¶å‡ºé”™: {e}\")\n",
    "            pass\n",
    "    \n",
    "    return True\n",
    "\n",
    "authorName_list, likeNr_list, URL_list, userURL_list = [], [], [], []\n",
    "qbar = tqdm(total=num, desc=\"å·²è·å–çš„ç¬”è®°æ•°é‡...\")\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦å·²ç»çˆ¬å–è¶³å¤Ÿæ•°é‡çš„ç¬”è®°ï¼Œæˆ–æ˜¯å¦å·²ç»è¾¾åˆ°é¡µé¢åº•éƒ¨\n",
    "while len(URL_list) < num:\n",
    "    if '- THE END -' in browser.page_source:\n",
    "        print(f\"å½“å‰ä¸{key_word}æœ‰å…³çš„ç¬”è®°æ•°é‡å°‘äº {num}\")\n",
    "        print('æ£€æŸ¥æ—¶é—´:', time.ctime())\n",
    "        break\n",
    "    \n",
    "    parsePage(browser.page_source, authorName_list, likeNr_list, URL_list, userURL_list, num)\n",
    "    qbar.update(1)\n",
    "\n",
    "    if len(URL_list) < num:\n",
    "        browser.execute_script('window.scrollTo(0,document.body.scrollHeight)')  # æ¨¡æ‹Ÿé¼ æ ‡æ»šåŠ¨\n",
    "        time.sleep(random.uniform(3, 5))\n",
    "\n",
    "if len(URL_list) > num:\n",
    "    URL_list = URL_list[:num]\n",
    "    authorName_list = authorName_list[:num]\n",
    "    likeNr_list = likeNr_list[:num]\n",
    "    userURL_list = userURL_list[:num]\n",
    "\n",
    "qbar.close()\n",
    "\n",
    "if all(len(lst) == num for lst in [authorName_list, likeNr_list, URL_list, userURL_list]):\n",
    "    print(f\"æ‰€æœ‰å±æ€§åˆ—è¡¨é•¿åº¦å‡ä¸º {num}ï¼Œçˆ¬å–æˆåŠŸï¼\")\n",
    "    print(f'æ£€æŸ¥æ—¶é—´:', time.ctime())\n",
    "else:\n",
    "    min_length = min(map(len, [authorName_list, likeNr_list, URL_list, userURL_list]))\n",
    "    print(f\"å½“å‰å±æ€§åˆ—è¡¨é•¿åº¦æœ€å°å€¼ä¸º {min_length}ï¼Œè¯·é‡æ–°è¿è¡Œä¸Šä¸€ä»£ç å•å…ƒï¼Œç›´è‡³æ‰€æœ‰å±æ€§åˆ—è¡¨é•¿åº¦å‡ä¸º {num}ï¼\")\n",
    "    print(f'æ£€æŸ¥æ—¶é—´:', time.ctime())\n",
    "\n",
    "# æ¸…æ´—æ•°æ®\n",
    "likeNr_list = extract_large_number(likeNr_list)\n",
    "URL_list = [re.sub(r'^/search_result/', '/', url) if url is not None else '' for url in URL_list]\n",
    "userURL_list = [url.split('/')[-1] if url is not None else '' for url in URL_list]\n",
    "\n",
    "print(\"ä»¥ä¸‹ä¸ºæ¸…æ´æ•°æ®ç¤ºä¾‹:\\n\")\n",
    "for i in range(min(3, len(authorName_list))):\n",
    "    print(\"author_name:\", authorName_list[i])\n",
    "    print(\"like_nr:\", likeNr_list[i])\n",
    "    print(\"url:\", URL_list[i])\n",
    "    print(\"user_url:\", userURL_list[i])\n",
    "    print(\"------\")\n",
    "\n",
    "def parse_note_page(browser, url, commentNr_list, content_list, datePublished_list, images_list, starNr_list):\n",
    "    \"\"\"\n",
    "    è§£æå•ä¸ªç¬”è®°é¡µé¢å¹¶æå–æ‰€éœ€æ•°æ®\n",
    "    \n",
    "    Args:\n",
    "        browser: Selenium WebDriver å®ä¾‹ï¼Œç”¨äºè·å–é¡µé¢å†…å®¹\n",
    "        url: ç¬”è®°çš„URL\n",
    "        commentNr_list (list): å­˜å‚¨è¯„è®ºæ•°é‡çš„åˆ—è¡¨\n",
    "        content_list (list): å­˜å‚¨ç¬”è®°å†…å®¹çš„åˆ—è¡¨\n",
    "        datePublished_list (list): å­˜å‚¨å‘å¸ƒæ—¶é—´çš„åˆ—è¡¨\n",
    "        images_list (list of list): å­˜å‚¨å›¾ç‰‡é“¾æ¥çš„åµŒå¥—åˆ—è¡¨\n",
    "        starNr_list (list): å­˜å‚¨æ”¶è—æ•°é‡çš„åˆ—è¡¨\n",
    "\n",
    "    Returns:\n",
    "        None: å°†æå–çš„æ•°æ®æ·»åŠ åˆ°ç›¸åº”çš„åˆ—è¡¨ä¸­\n",
    "    \"\"\"\n",
    "    whole_url = 'https://www.xiaohongshu.com/explore' + url  # æ„é€ å®Œæ•´ç¬”è®°URL\n",
    "    browser.get(whole_url)\n",
    "    WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@name=\"description\"]')))  # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ\n",
    "    html = browser.page_source\n",
    "    selector = Selector(text=html)\n",
    "    \n",
    "    try:\n",
    "        # é€‰æ‹©å¹¶æå–ç½‘é¡µæ•°æ®\n",
    "        comment_nr = selector.xpath('//*[@class=\"total\"]/text()').get()  # è¯„è®ºæ•°é‡\n",
    "        content = selector.xpath('//*[@name=\"description\"]/@content').get()  # å†…å®¹\n",
    "        datePublished = selector.xpath('//*[@class=\"date\"]/text()').get()  # å‘å¸ƒæ—¶é—´\n",
    "        \n",
    "        # æå–å›¾åƒé“¾æ¥ï¼ˆä¸»å›¾å’Œå¤´åƒï¼‰\n",
    "        main_image = selector.xpath('.//a[contains(@class, \"cover\")]/img/@src').get()\n",
    "        avatar_image = selector.xpath('.//a[contains(@class, \"author\")]/img/@src').get()\n",
    "        images = [main_image, avatar_image] if main_image and avatar_image else []\n",
    "        \n",
    "        star_nr = selector.xpath('//*[@class=\"count\"]/text()').get()  # æ”¶è—æ•°é‡\n",
    "\n",
    "        commentNr_list.append(comment_nr)\n",
    "        content_list.append(content)\n",
    "        datePublished_list.append(datePublished)\n",
    "        images_list.append(images)\n",
    "        starNr_list.append(star_nr)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"è§£æç¬”è®°é¡µé¢æ—¶å‡ºé”™: {e}\")\n",
    "        commentNr_list.append(None)\n",
    "        content_list.append(None)\n",
    "        datePublished_list.append(None)\n",
    "        images_list.append([])\n",
    "        starNr_list.append(None)\n",
    "        \n",
    "commentNr_list, content_list, datePublished_list, images_list, starNr_list = [], [], [], [], []\n",
    "\n",
    "qbar = tqdm(total=len(URL_list), desc=\"å·²è·å–çš„ç¬”è®°æ•°é‡...\")\n",
    "for url in URL_list:\n",
    "    parse_note_page(browser, url, commentNr_list, content_list, datePublished_list, images_list, starNr_list)\n",
    "    qbar.update(1)\n",
    "    time.sleep(random.uniform(0.5, 2))\n",
    "\n",
    "qbar.close()\n",
    "\n",
    "if all(len(lst) == num for lst in [commentNr_list, content_list, datePublished_list, images_list, starNr_list]):\n",
    "    print(f\"æ‰€æœ‰å±æ€§åˆ—è¡¨é•¿åº¦å‡ä¸º {num}ï¼Œçˆ¬å–æˆåŠŸï¼\")\n",
    "    print(f'æ£€æŸ¥æ—¶é—´:', time.ctime())\n",
    "else:\n",
    "    min_length = min(map(len, [commentNr_list, content_list, datePublished_list, images_list, starNr_list]))\n",
    "    print(f\"å½“å‰å±æ€§åˆ—è¡¨é•¿åº¦æœ€å°å€¼ä¸º {min_length}ï¼Œè¯·é‡æ–°è¿è¡Œä¸Šä¸€ä»£ç å•å…ƒï¼Œç›´è‡³æ‰€æœ‰å±æ€§åˆ—è¡¨é•¿åº¦å‡ä¸º {num}ï¼\")\n",
    "    print(f'æ£€æŸ¥æ—¶é—´:', time.ctime())\n",
    "\n",
    "# æ¸…æ´—æ›´å¤šæ•°æ®\n",
    "commentNr_list = extract_large_number(extract_number(commentNr_list))\n",
    "starNr_list = extract_large_number(starNr_list)\n",
    "datePublished_list = extract_date(datePublished_list)\n",
    "\n",
    "print(\"ä»¥ä¸‹ä¸ºæ¸…æ´æ•°æ®ç¤ºä¾‹:\\n\")\n",
    "for i in range(min(3, len(commentNr_list))):\n",
    "    print(\"comment_nr:\", commentNr_list[i])\n",
    "    print(\"content:\", content_list[i])\n",
    "    print(\"datePublished:\", datePublished_list[i])\n",
    "    print(\"images:\", images_list[i])\n",
    "    print(\"star_nr:\", starNr_list[i])\n",
    "    print(\"------\")\n",
    "\n",
    "# å‡½æ•°ï¼šæ‹¦æˆªå¹¶ä¸‹è½½å›¾åƒ\n",
    "def download_images_with_selenium_wire(browser, images_list, folder_path='images'):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Selenium Wire æ‹¦æˆªå¹¶ä¸‹è½½å›¾åƒã€‚\n",
    "\n",
    "    Args:\n",
    "        browser: Selenium Wire WebDriver å®ä¾‹ã€‚\n",
    "        images_list (list of list): åŒ…å«å›¾åƒURLçš„åµŒå¥—åˆ—è¡¨ã€‚\n",
    "        folder_path (str): å›¾åƒä¿å­˜çš„ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„ã€‚\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # å°†æ‰€æœ‰å›¾åƒURLæ‰å¹³åŒ–ä¸ºä¸€ä¸ªåˆ—è¡¨\n",
    "    flat_images = [img_url for sublist in images_list for img_url in sublist if img_url]\n",
    "\n",
    "    # ä½¿ç”¨é›†åˆå»é‡ï¼Œé¿å…é‡å¤ä¸‹è½½åŒä¸€å¼ å›¾åƒ\n",
    "    unique_images = list(set(flat_images))\n",
    "\n",
    "    print(f\"å…±æ‰¾åˆ° {len(unique_images)} å¼ å”¯ä¸€å›¾åƒéœ€è¦ä¸‹è½½ã€‚\")\n",
    "\n",
    "    for img_url in tqdm(unique_images, desc=\"ä¸‹è½½å›¾åƒ\"):\n",
    "        try:\n",
    "            # æ¸…é™¤ä¹‹å‰çš„è¯·æ±‚è®°å½•\n",
    "            browser.scopes = ['.*']  # åªæ‹¦æˆªæ‰€æœ‰è¯·æ±‚\n",
    "            browser.requests.clear()\n",
    "\n",
    "            # å¯¼èˆªåˆ°å›¾åƒURL\n",
    "            browser.get(img_url)\n",
    "\n",
    "            # ç­‰å¾…å›¾åƒå“åº”\n",
    "            for request in browser.requests:\n",
    "                if request.response and request.url == img_url and 'image' in request.response.headers.get('Content-Type', ''):\n",
    "                    image_data = request.response.body\n",
    "\n",
    "                    # ç¡®å®šå›¾ç‰‡çš„æ‰©å±•å\n",
    "                    img_extension = os.path.splitext(img_url.split('?')[0])[1]\n",
    "                    if img_extension.lower() not in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']:\n",
    "                        img_extension = '.jpg'  # é»˜è®¤æ‰©å±•å\n",
    "\n",
    "                    # æ„é€ å›¾ç‰‡æ–‡ä»¶åï¼Œé¿å…é‡å¤\n",
    "                    img_filename = f\"{hash(img_url)}{img_extension}\"\n",
    "                    img_path = os.path.join(folder_path, img_filename)\n",
    "\n",
    "                    # ä¿å­˜å›¾ç‰‡åˆ°æœ¬åœ°\n",
    "                    with open(img_path, 'wb') as f:\n",
    "                        f.write(image_data)\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"ä¸‹è½½å›¾åƒ {img_url} æ—¶å‡ºé”™ï¼š{e}\")\n",
    "\n",
    "# ä¸‹è½½å›¾åƒ\n",
    "download_images_with_selenium_wire(browser, images_list, images_folder)\n",
    "\n",
    "# æ„å»ºæ•°æ®å­—å…¸\n",
    "dic = {\n",
    "    # \"author_collect_nr\": authorCollectNr_list,# ä½œè€…è·èµä¸æ”¶è—æ•°é‡\n",
    "    # \"author_fans_nr\": authorFansNr_list,# ç²‰ä¸æ•°é‡\n",
    "    \"author_name\": authorName_list,  # ä½œè€…åå­—\n",
    "    # \"author_note_nr\": authorNoteNr_list,# ä½œè€…ç¬”è®°æ•°é‡\n",
    "    \"comment_nr\": commentNr_list,  # ç¬”è®°è¯„è®ºæ•°é‡\n",
    "    \"content\": content_list,  # ç¬”è®°å†…å®¹\n",
    "    \"datePublished\": datePublished_list,  # ç¬”è®°å‘å¸ƒæ—¥æœŸ\n",
    "    \"images\": images_list,  # ç¬”è®°å°é¢å›¾ç‰‡ï¼ˆåµŒå¥—åˆ—è¡¨ï¼‰\n",
    "    \"like_nr\": likeNr_list,  # ç¬”è®°è·èµæ•°é‡\n",
    "    \"star_nr\": starNr_list,\n",
    "    \"url\": URL_list,  # ç¬”è®°URL\n",
    "    \"user_url\": userURL_list  # ä½œè€…URL\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(dic)\n",
    "df = df.explode('images')  # å±•å¼€åµŒå¥—çš„å›¾åƒåˆ—è¡¨\n",
    "df = df[~df.duplicated(keep='first')]  # æ£€ç´¢å¹¶åˆ é™¤æ‰€æœ‰å±æ€§å€¼éƒ½ç›¸åŒçš„è¡Œ,å³ä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„è¡Œï¼Œåˆ é™¤åç»­çš„é‡å¤è¡Œ\n",
    "print(\"åˆ é™¤\", num - len(df), \"è¡Œé‡å¤è¡Œåå‰©ä½™\", len(df), \"è¡Œã€‚\")\n",
    "print('æ£€æŸ¥æ—¶é—´:', time.ctime())\n",
    "display(df.head(10))\n",
    "df.to_csv('scraped_data.csv', encoding='utf-8-sig')\n",
    "print(\"æ•°æ®å·²ä¿å­˜åˆ° 'scraped_data.csv'\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xiaoliu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
