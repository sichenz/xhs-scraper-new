{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "即将开始检查小红书登录状态...\n",
      "爬取数据有账户封禁的风险，建议使用非主账号登录。\n",
      "登录成功\n",
      "检查时间: Thu Dec  5 01:59:09 2024\n",
      "即将开始检查网页加载状态...\n",
      "如果网页进入人机验证页面，请先手动完成验证。\n",
      "请在文本框中根据提示输入搜索关键词和笔记爬取数量。\n",
      "加载成功\n",
      "检查时间: Thu Dec  5 01:59:14 2024\n",
      "已自动更改模式为图文。\n",
      "请在文本框中根据提示输入对应数字来选择排序方式。\n",
      "1.综合\n",
      "2.最新\n",
      "3.最热\n",
      "已选择排序方式为: 综合\n",
      "检查时间: Thu Dec  5 01:59:19 2024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9726caa002c4beea9e621f718f41d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "爬取进度:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前URL: https://www.xiaohongshu.com/search_result?keyword=%E4%B8%8A%E7%BA%BD%E5%B8%82%E5%9C%BA%E8%90%A5%E9%94%80&source=web_explore_feed&type=51\n",
      "已导航到搜索结果页面。\n",
      "找到元素，选择器: //*[contains(text(), '图文')]\n",
      "正在分析页面结构...\n",
      "找到容器，xpath: //div[contains(@class, \"note-item\")]\n",
      "找到 18 个内容元素\n",
      "当前已爬取总数: 17\n",
      "正在分析页面结构...\n",
      "找到容器，xpath: //div[contains(@class, \"note-item\")]\n",
      "找到 18 个内容元素\n",
      "当前已爬取总数: 28\n",
      "正在分析页面结构...\n",
      "找到容器，xpath: //div[contains(@class, \"note-item\")]\n",
      "找到 18 个内容元素\n",
      "当前已爬取总数: 38\n",
      "正在分析页面结构...\n",
      "找到容器，xpath: //div[contains(@class, \"note-item\")]\n",
      "找到 18 个内容元素\n",
      "当前已爬取总数: 46\n",
      "正在分析页面结构...\n",
      "找到容器，xpath: //div[contains(@class, \"note-item\")]\n",
      "找到 18 个内容元素\n",
      "当前已爬取总数: 55\n",
      "正在分析页面结构...\n",
      "找到容器，xpath: //div[contains(@class, \"note-item\")]\n",
      "找到 18 个内容元素\n",
      "当前已爬取总数: 64\n",
      "正在分析页面结构...\n",
      "找到容器，xpath: //div[contains(@class, \"note-item\")]\n",
      "找到 18 个内容元素\n",
      "当前已爬取总数: 73\n",
      "正在分析页面结构...\n",
      "找到容器，xpath: //div[contains(@class, \"note-item\")]\n",
      "找到 18 个内容元素\n",
      "当前已爬取总数: 82\n",
      "正在分析页面结构...\n",
      "找到容器，xpath: //div[contains(@class, \"note-item\")]\n",
      "找到 18 个内容元素\n",
      "当前已爬取总数: 91\n",
      "正在分析页面结构...\n",
      "找到容器，xpath: //div[contains(@class, \"note-item\")]\n",
      "找到 18 个内容元素\n",
      "当前已爬取总数: 101\n",
      "总共收集的条目数: 101\n",
      "收集的数据样本:\n",
      "URL: 6572c4b3000000003a00b725\n",
      "URL: 62272e89000000002103ce15\n",
      "URL: 664e11e90000000016013819\n",
      "URL: 6579a9ca0000000006020dfa\n",
      "URL: 6703a702000000001902d59d\n",
      "截断后的总条目数: 100\n",
      "收集的数据样本:\n",
      "作者: 心莫鸢, 点赞: 1, URL: 6572c4b3000000003a00b725\n",
      "作者: 美研老阿姨Eva讲申请, 点赞: 66, URL: 62272e89000000002103ce15\n",
      "作者: Iris林小西🐱🌸, 点赞: 43, URL: 664e11e90000000016013819\n",
      "作者: Camellia, 点赞: 30, URL: 6579a9ca0000000006020dfa\n",
      "作者: 邵主任讲升学, 点赞: 5, URL: 6703a702000000001902d59d\n",
      "开始提取附加字段，包括帖子内容、日期发布和评论数量...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a789c4c1cb4e278d1b67160aa7dcec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "已获取的笔记数量...:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已提取附加字段，笔记ID: 6572c4b3000000003a00b725\n",
      "已提取附加字段，笔记ID: 62272e89000000002103ce15\n",
      "已提取附加字段，笔记ID: 664e11e90000000016013819\n",
      "已提取附加字段，笔记ID: 6579a9ca0000000006020dfa\n",
      "已提取附加字段，笔记ID: 6703a702000000001902d59d\n",
      "已提取附加字段，笔记ID: 658bf47b000000001101c78b\n",
      "已提取附加字段，笔记ID: 6473d619000000001300603d\n",
      "已提取附加字段，笔记ID: 66f5ea02000000002c01718c\n",
      "已提取附加字段，笔记ID: 66a0f883000000000d00d341\n",
      "已提取附加字段，笔记ID: 657c16570000000016004d4a\n",
      "已提取附加字段，笔记ID: 673daa98000000000202bef1\n",
      "已提取附加字段，笔记ID: 64dcd0b40000000010032ecc\n",
      "已提取附加字段，笔记ID: 668691fb000000001c024430\n",
      "已提取附加字段，笔记ID: 667cd95b000000001e0123b8\n",
      "已提取附加字段，笔记ID: 667a99c4000000001c023d4a\n",
      "已提取附加字段，笔记ID: 61ffcac0000000000102d6c9\n",
      "已提取附加字段，笔记ID: 66d19d18000000001d03984b\n",
      "已提取附加字段，笔记ID: 65b362b2000000002b03f8e3\n",
      "已提取附加字段，笔记ID: 67283993000000001901aa51\n",
      "已提取附加字段，笔记ID: 668688ad000000000d00ff62\n",
      "已提取附加字段，笔记ID: 66c32746000000001d01514c\n",
      "已提取附加字段，笔记ID: 649908ed00000000140276ca\n",
      "已提取附加字段，笔记ID: 67492679000000000202a8ab\n",
      "已提取附加字段，笔记ID: 6745574f0000000006015cee\n",
      "已提取附加字段，笔记ID: 63d67794000000001b01ca8a\n",
      "已提取附加字段，笔记ID: 624b001b000000000102a75d\n",
      "已提取附加字段，笔记ID: 6708b6c2000000002c029eab\n",
      "已提取附加字段，笔记ID: 6744456400000000070355cd\n",
      "已提取附加字段，笔记ID: 66d9ba150000000012012696\n",
      "已提取附加字段，笔记ID: 63f519ad00000000130369f7\n",
      "已提取附加字段，笔记ID: 657d11b10000000034036eea\n",
      "已提取附加字段，笔记ID: 6698f6050000000025014d09\n",
      "已提取附加字段，笔记ID: 66fe7983000000002c0142f7\n",
      "已提取附加字段，笔记ID: 641af9c70000000027003721\n",
      "已提取附加字段，笔记ID: 66573987000000000f00c4c4\n",
      "已提取附加字段，笔记ID: 672ad32b000000001b02d2a9\n",
      "已提取附加字段，笔记ID: 630d8d96000000000900c13c\n",
      "已提取附加字段，笔记ID: 6728ceaf0000000019019028\n",
      "已提取附加字段，笔记ID: 66335f30000000001e02f7c0\n",
      "已提取附加字段，笔记ID: 66cac6b6000000001f01e2b1\n",
      "已提取附加字段，笔记ID: 67244e7e000000001b02e3bf\n",
      "已提取附加字段，笔记ID: 671f9bf200000000210090fd\n",
      "已提取附加字段，笔记ID: 67176aae000000002100b6b3\n",
      "已提取附加字段，笔记ID: 657a7b53000000003a00ef7c\n",
      "已提取附加字段，笔记ID: 670cff450000000026035c14\n",
      "已提取附加字段，笔记ID: 667e84b4000000001f007cca\n",
      "已提取附加字段，笔记ID: 662e3216000000001c0054b5\n",
      "已提取附加字段，笔记ID: 672ee113000000001d03a05b\n",
      "已提取附加字段，笔记ID: 65ba3190000000001100f694\n",
      "已提取附加字段，笔记ID: 650c1047000000002000206b\n",
      "已提取附加字段，笔记ID: 65b36da40000000010039cee\n",
      "已提取附加字段，笔记ID: 65e825c600000000040025e0\n",
      "已提取附加字段，笔记ID: 6369adf8000000000602ab4c\n",
      "已提取附加字段，笔记ID: 67451fd20000000007036d8f\n",
      "已提取附加字段，笔记ID: 670ce8060000000021000be0\n",
      "已提取附加字段，笔记ID: 64bbe418000000001700eaa3\n",
      "已提取附加字段，笔记ID: 6436a4140000000013033ea7\n",
      "已提取附加字段，笔记ID: 65aa2ea9000000002c012c5f\n",
      "已提取附加字段，笔记ID: 66c5f4be000000001f0181e0\n",
      "已提取附加字段，笔记ID: 67467a42000000000202b4dc\n",
      "已提取附加字段，笔记ID: 666c4573000000000d00cb24\n",
      "已提取附加字段，笔记ID: 656efb1c0000000009023ea9\n",
      "已提取附加字段，笔记ID: 6703e017000000001a0211b6\n",
      "已提取附加字段，笔记ID: 66092e63000000001a00d7e8\n",
      "已提取附加字段，笔记ID: 65e9274a000000000102afa2\n",
      "已提取附加字段，笔记ID: 645b391100000000270010fb\n",
      "已提取附加字段，笔记ID: 673ae15f000000000203a910\n",
      "已提取附加字段，笔记ID: 66fa2ed9000000001b02103e\n",
      "已提取附加字段，笔记ID: 67444bb3000000000202c47d\n",
      "已提取附加字段，笔记ID: 667671b2000000001f0076d0\n",
      "已提取附加字段，笔记ID: 6524f621000000001e03d06a\n",
      "已提取附加字段，笔记ID: 673f4a74000000000800497e\n",
      "已提取附加字段，笔记ID: 640ab3b80000000027011865\n",
      "已提取附加字段，笔记ID: 66b76d33000000001e01d8bd\n",
      "已提取附加字段，笔记ID: 671f123e000000001b010024\n",
      "已提取附加字段，笔记ID: 657fbb76000000001502f179\n",
      "已提取附加字段，笔记ID: 65d89361000000000b0170da\n",
      "已提取附加字段，笔记ID: 673c57ba0000000007030d29\n",
      "已提取附加字段，笔记ID: 673b1932000000000702583c\n",
      "已提取附加字段，笔记ID: 6746c480000000000202fa23\n",
      "已提取附加字段，笔记ID: 6666babc00000000150115b1\n",
      "已提取附加字段，笔记ID: 673f9ab8000000000703bce7\n",
      "已提取附加字段，笔记ID: 65e27f1f0000000004000638\n",
      "已提取附加字段，笔记ID: 63e0544800000000040054e6\n",
      "已提取附加字段，笔记ID: 666da534000000001d015725\n",
      "已提取附加字段，笔记ID: 66472afa0000000015012485\n",
      "已提取附加字段，笔记ID: 668e065300000000250057d3\n",
      "已提取附加字段，笔记ID: 66b35829000000001e01a530\n",
      "已提取附加字段，笔记ID: 6659534f0000000016010d7d\n",
      "已提取附加字段，笔记ID: 66964eec00000000250003a5\n",
      "已提取附加字段，笔记ID: 66dabd5c000000000c0182cc\n",
      "已提取附加字段，笔记ID: 66ea9c0900000000120135fb\n",
      "已提取附加字段，笔记ID: 63e3382800000000040051f8\n",
      "已提取附加字段，笔记ID: 66752238000000001e011ac9\n",
      "已提取附加字段，笔记ID: 67209767000000001a01fbec\n",
      "已提取附加字段，笔记ID: 67468c5b000000000703bb42\n",
      "已提取附加字段，笔记ID: 661cac47000000000d0321be\n",
      "已提取附加字段，笔记ID: 65d6f24d00000000070062fb\n",
      "已提取附加字段，笔记ID: 6724aa76000000001b028eea\n",
      "已提取附加字段，笔记ID: 663f49ca000000001e0327bf\n",
      "开始下载主图片和头像图片...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed05a5c763da4c499a41c5cc67ce4d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "下载主图片:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb452dba43f4a2dab57b4493f5f1a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "下载头像图片:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有图片下载完成。\n",
      "                          Author Name Likes Comments  \\\n",
      "URL                                                    \n",
      "6572c4b3000000003a00b725          心莫鸢     1        0   \n",
      "62272e89000000002103ce15  美研老阿姨Eva讲申请    66        0   \n",
      "664e11e90000000016013819    Iris林小西🐱🌸    43        0   \n",
      "6579a9ca0000000006020dfa     Camellia    30        0   \n",
      "6703a702000000001902d59d       邵主任讲升学     5        0   \n",
      "\n",
      "                                        Post Title          Caption  \\\n",
      "URL                                                                   \n",
      "6572c4b3000000003a00b725        上纽市场营销和零售offer火热出炉  3 亿人的生活经验，都在小红书   \n",
      "62272e89000000002103ce15    上海纽约大学项目解析:营销与零售科学MMRS  3 亿人的生活经验，都在小红书   \n",
      "664e11e90000000016013819       毕业季太卷了，上纽大的营销实操值得表扬  3 亿人的生活经验，都在小红书   \n",
      "6579a9ca0000000006020dfa  Go violet💜第一个offer是NYU的！  3 亿人的生活经验，都在小红书   \n",
      "6703a702000000001902d59d        3.上海纽约大学目前开设了哪些专业？  3 亿人的生活经验，都在小红书   \n",
      "\n",
      "                         Date Published                               Images  \\\n",
      "URL                                                                            \n",
      "6572c4b3000000003a00b725            N/A  images/6572c4b3000000003a00b725.jpg   \n",
      "62272e89000000002103ce15            N/A  images/62272e89000000002103ce15.jpg   \n",
      "664e11e90000000016013819            N/A  images/664e11e90000000016013819.jpg   \n",
      "6579a9ca0000000006020dfa            N/A  images/6579a9ca0000000006020dfa.jpg   \n",
      "6703a702000000001902d59d            N/A  images/6703a702000000001902d59d.jpg   \n",
      "\n",
      "                                                 Author Avatar Stars  \\\n",
      "URL                                                                    \n",
      "6572c4b3000000003a00b725  avatars/6572c4b3000000003a00b725.jpg     1   \n",
      "62272e89000000002103ce15  avatars/62272e89000000002103ce15.jpg     1   \n",
      "664e11e90000000016013819  avatars/664e11e90000000016013819.jpg     1   \n",
      "6579a9ca0000000006020dfa  avatars/6579a9ca0000000006020dfa.jpg     1   \n",
      "6703a702000000001902d59d  avatars/6703a702000000001902d59d.jpg     1   \n",
      "\n",
      "                         Author Collect Nr Author Fans Nr Author Note Nr  \\\n",
      "URL                                                                        \n",
      "6572c4b3000000003a00b725                 0              0              0   \n",
      "62272e89000000002103ce15                 0              0              0   \n",
      "664e11e90000000016013819                 0              0              0   \n",
      "6579a9ca0000000006020dfa                 0              0              0   \n",
      "6703a702000000001902d59d                 0              0              0   \n",
      "\n",
      "                         Video URL  \\\n",
      "URL                                  \n",
      "6572c4b3000000003a00b725       N/A   \n",
      "62272e89000000002103ce15       N/A   \n",
      "664e11e90000000016013819       N/A   \n",
      "6579a9ca0000000006020dfa       N/A   \n",
      "6703a702000000001902d59d       N/A   \n",
      "\n",
      "                                                                   User URL  \n",
      "URL                                                                          \n",
      "6572c4b3000000003a00b725  /search_result/6572c4b3000000003a00b725?xsec_t...  \n",
      "62272e89000000002103ce15  /search_result/62272e89000000002103ce15?xsec_t...  \n",
      "664e11e90000000016013819  /search_result/664e11e90000000016013819?xsec_t...  \n",
      "6579a9ca0000000006020dfa  /search_result/6579a9ca0000000006020dfa?xsec_t...  \n",
      "6703a702000000001902d59d  /search_result/6703a702000000001902d59d?xsec_t...  \n",
      "数据已保存到 'scraped_data2.csv'\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的包\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import requests  # 新增：用于下载图片\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "# 配置Chrome浏览器\n",
    "options = Options()\n",
    "options.add_experimental_option('debuggerAddress', '127.0.0.1:9222')\n",
    "options.add_argument('--incognito')\n",
    "browser = webdriver.Chrome(options=options)\n",
    "action = ActionChains(browser)\n",
    "\n",
    "# 定义全局变量\n",
    "key_word = \"\"\n",
    "num = 0\n",
    "\n",
    "def selenium_test():\n",
    "    \"\"\"\n",
    "    初始化Selenium浏览器，检查登录状态，并获取用户输入的搜索关键词和爬取数量。\n",
    "    \"\"\"\n",
    "    global key_word, num  # 使用全局变量\n",
    "    browser.get('https://www.xiaohongshu.com/explore')\n",
    "\n",
    "    print(\"即将开始检查小红书登录状态...\")\n",
    "    print(\"爬取数据有账户封禁的风险，建议使用非主账号登录。\")\n",
    "    \n",
    "    wait = WebDriverWait(browser, 30)  # 设置最长等待时间为30秒\n",
    "\n",
    "    # 定义循环：检查是否成功登录小红书\n",
    "    while True:\n",
    "        try:\n",
    "            # 尝试查找登录相关的元素\n",
    "            login_element = browser.find_element(By.XPATH, \"//div[contains(text(), '登录探索更多内容')]\")\n",
    "            print('暂未登录，请手动登录')\n",
    "            print('检查时间:', time.ctime())\n",
    "            wait.until(EC.staleness_of(login_element))  # 等待登录状态改变\n",
    "        except:\n",
    "            print('登录成功')\n",
    "            print('检查时间:', time.ctime())\n",
    "            break\n",
    "\n",
    "    print(\"即将开始检查网页加载状态...\")\n",
    "    print(\"如果网页进入人机验证页面，请先手动完成验证。\")\n",
    "    print(\"请在文本框中根据提示输入搜索关键词和笔记爬取数量。\")\n",
    "    key_word = input(\"搜索关键词：\")\n",
    "    num = input(\"笔记爬取数量：\")\n",
    "    try:\n",
    "        num = int(num)\n",
    "    except ValueError:\n",
    "        print(\"请输入一个有效的数字作为笔记爬取数量。\")\n",
    "        browser.quit()\n",
    "        exit()\n",
    "\n",
    "    url = f'https://www.xiaohongshu.com/search_result?keyword={key_word}&source=web_explore_feed'\n",
    "    browser.get(url)\n",
    "\n",
    "    try:\n",
    "        # 等待标题中包含关键词\n",
    "        wait.until(EC.title_contains(key_word))\n",
    "        print('加载成功')\n",
    "        print('检查时间:', time.ctime())\n",
    "    except:\n",
    "        print('页面加载超时，请检查网络或关键词。')\n",
    "        browser.quit()\n",
    "        exit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    selenium_test()\n",
    "\n",
    "# 通过模拟点击更改模式\n",
    "try:\n",
    "    mode = WebDriverWait(browser, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"search-type\"]/div/div/div[2]'))\n",
    "    )\n",
    "    mode.click()  # 模拟鼠标点击\n",
    "    print('已自动更改模式为图文。')\n",
    "except Exception as e:\n",
    "    print(f\"更改模式时出错: {e}\")\n",
    "\n",
    "# 通过模拟点击更改排序方式\n",
    "sort_order = ['综合', '最新', '最热']\n",
    "print(\"请在文本框中根据提示输入对应数字来选择排序方式。\")\n",
    "for i, order in enumerate(sort_order, 1):\n",
    "    print(f'{i}.{order}')\n",
    "selected_order = input(\"请选择排序方式:\")\n",
    "\n",
    "try:\n",
    "    selected_order_index = int(selected_order)\n",
    "    if selected_order_index < 1 or selected_order_index > len(sort_order):\n",
    "        raise ValueError\n",
    "    selected_order_text = sort_order[selected_order_index - 1]\n",
    "    element = browser.find_element(By.XPATH, '//*[@id=\"global\"]/div[2]/div[2]/div/div[1]/div[2]')\n",
    "    action.move_to_element(element).perform()  # 模拟鼠标悬停\n",
    "    menu = browser.find_element(By.CLASS_NAME, 'dropdown-items')\n",
    "    option = menu.find_element(By.XPATH, f'/html/body/div[4]/div/li[{selected_order_index}]')\n",
    "    option.click()  # 模拟鼠标点击\n",
    "    print(f'已选择排序方式为: {selected_order_text}')\n",
    "    print('检查时间:', time.ctime())\n",
    "except (ValueError, IndexError):\n",
    "    print(\"选择排序方式时输入无效。\")\n",
    "except Exception as e:\n",
    "    print(f\"选择排序方式时出错: {e}\")\n",
    "\n",
    "# 初始化数据存储列表\n",
    "authorName_list = []\n",
    "likeNr_list = []\n",
    "URL_list = []\n",
    "userURL_list = []\n",
    "commentNr_list = []\n",
    "post_title_list = [] \n",
    "caption_list = []  # 已初始化，用于存储帖子内容\n",
    "datePublished_list = []\n",
    "images_list = []\n",
    "author_avatar_list = []  # 单独初始化用于存储头像图片\n",
    "starNr_list = []\n",
    "authorCollectNr_list = []\n",
    "authorFansNr_list = []\n",
    "authorNoteNr_list = []\n",
    "video_urls = [] \n",
    "\n",
    "def parsePage(page_source):\n",
    "    \"\"\"\n",
    "    解析当前页面的HTML内容，提取笔记的基本信息并更新对应的列表。\n",
    "    \n",
    "    Args:\n",
    "        page_source (str): 当前页面的HTML内容\n",
    "    \"\"\"\n",
    "    response = TextResponse(url=browser.current_url, body=page_source.encode('utf-8'), encoding='utf-8')\n",
    "    selector = Selector(response)\n",
    "\n",
    "    print(\"正在分析页面结构...\")\n",
    "\n",
    "    containers = [\n",
    "        '//div[contains(@class, \"note-item\")]'\n",
    "    ]\n",
    "\n",
    "    for container in containers:\n",
    "        elements = selector.xpath(container)\n",
    "        if elements:\n",
    "            print(f\"找到容器，xpath: {container}\")\n",
    "\n",
    "    content_elements = selector.xpath('//section[contains(@class, \"note-item\")]')\n",
    "    if content_elements:\n",
    "        print(f\"找到 {len(content_elements)} 个内容元素\")\n",
    "\n",
    "        for element in content_elements:\n",
    "            try:\n",
    "                # 提取用户URL\n",
    "                user_url = element.xpath('.//a[contains(@class, \"cover\")]/@href').get()\n",
    "                if user_url:\n",
    "                    note_id = user_url.split('/')[-1].split('?')[0]\n",
    "                    if note_id in URL_list:\n",
    "                        continue  # 避免重复\n",
    "                    URL_list.append(note_id)\n",
    "                    userURL_list.append(user_url)\n",
    "\n",
    "                    # 提取作者名字\n",
    "                    author = element.xpath('.//div[contains(@class, \"author-wrapper\")]//span[contains(@class, \"name\")]/text()').get()\n",
    "                    authorName_list.append(author.strip() if author else \"N/A\")\n",
    "\n",
    "                    # 提取点赞数量\n",
    "                    likes = element.xpath('.//span[contains(@class, \"like-wrapper\")]/span[contains(@class, \"count\")]/text()').get()\n",
    "                    likeNr_list.append(likes.strip() if likes else \"0\")\n",
    "\n",
    "                    # 提取帖子标题\n",
    "                    post_title = element.xpath('.//a[contains(@class, \"title\")]//span/text()').getall()\n",
    "                    post_title_cleaned = ' '.join([c.strip() for c in post_title if c.strip()])\n",
    "                    post_title_list.append(post_title_cleaned if post_title_cleaned else \"N/A\")\n",
    "\n",
    "                    # 提取图片（主图）\n",
    "                    main_image = element.xpath('.//a[contains(@class, \"cover\")]/img/@src').get()\n",
    "                    images_list.append(main_image.strip() if main_image else \"N/A\")\n",
    "\n",
    "                    # 提取头像图片\n",
    "                    avatar_image = element.xpath('.//a[contains(@class, \"author\")]/img/@src').get()\n",
    "                    author_avatar_list.append(avatar_image.strip() if avatar_image else \"N/A\")\n",
    "\n",
    "                    # 初始化附加字段的默认值\n",
    "                    commentNr_list.append(\"0\")\n",
    "                    datePublished_list.append(\"N/A\")\n",
    "                    starNr_list.append(\"0\")\n",
    "                    authorCollectNr_list.append(\"0\")\n",
    "                    authorFansNr_list.append(\"0\")\n",
    "                    authorNoteNr_list.append(\"0\")\n",
    "                    video_urls.append(\"N/A\")  \n",
    "                    caption_list.append(\"N/A\")  # 初始化为默认值\n",
    "\n",
    "                    qbar.update(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"处理元素时出错: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"当前已爬取总数: {len(URL_list)}\")\n",
    "\n",
    "def extract_additional_fields(note_url, note_id):\n",
    "    \"\"\"\n",
    "    访问每个笔记的页面，提取附加字段，包括评论数量、发布时间、收藏数量、粉丝数量、笔记数量、视频URL以及帖子内容（Caption）。\n",
    "    \n",
    "    Args:\n",
    "        note_url (str): 笔记的相对URL\n",
    "        note_id (str): 笔记的唯一ID\n",
    "    \"\"\"\n",
    "    try:\n",
    "        full_note_url = f'https://www.xiaohongshu.com{note_url}'\n",
    "        browser.get(full_note_url)\n",
    "        \n",
    "        # 使用显式等待代替固定的等待时间，等待描述meta标签加载完成\n",
    "        wait = WebDriverWait(browser, 15)\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, '//*[@name=\"description\"]')))\n",
    "        \n",
    "        # 获取页面源代码\n",
    "        page_source = browser.page_source\n",
    "        response = TextResponse(url=browser.current_url, body=page_source.encode('utf-8'), encoding='utf-8')\n",
    "        selector = Selector(response)\n",
    "\n",
    "        # 提取评论数量\n",
    "        comments = selector.xpath('//*[@class=\"total\"]/text()').get()\n",
    "        comments = comments.strip() if comments else \"0\"\n",
    "\n",
    "        # 提取发布时间，使用多个XPath策略\n",
    "        date_published = selector.xpath('//*[@class=\"date\"]/text()').get()\n",
    "        if not date_published:\n",
    "            # 替代XPath，如果第一个失败\n",
    "            date_published = selector.xpath('//time/@datetime').get()\n",
    "        date_published = date_published.strip() if date_published else \"N/A\"\n",
    "\n",
    "        # 提取收藏数量\n",
    "        stars = selector.xpath('//*[@class=\"count\"]/text()').get()\n",
    "        stars = stars.strip() if stars else \"0\"\n",
    "\n",
    "        # 提取作者收藏数量\n",
    "        collect_nr = selector.xpath('//span[contains(@class, \"collect\") or contains(@class, \"saved\")]/text()').get()\n",
    "        collect_nr = collect_nr.strip() if collect_nr else \"0\"\n",
    "\n",
    "        # 提取作者粉丝数量\n",
    "        fans_nr = selector.xpath('//span[contains(@class, \"fans\") or contains(@class, \"followers\")]/text()').get()\n",
    "        fans_nr = fans_nr.strip() if fans_nr else \"0\"\n",
    "\n",
    "        # 提取作者笔记数量\n",
    "        note_nr = selector.xpath('//span[contains(@class, \"notes\") or contains(@class, \"posts\")]/text()').get()\n",
    "        note_nr = note_nr.strip() if note_nr else \"0\"\n",
    "\n",
    "        # 提取视频URL（如果有）\n",
    "        video_url = selector.xpath('//video/@src').get()\n",
    "        video_url = video_url.strip() if video_url else \"N/A\"\n",
    "\n",
    "        # 提取帖子内容（Caption）\n",
    "        caption = selector.xpath('//*[@name=\"description\"]/@content').get()\n",
    "        caption = caption.strip() if caption else \"N/A\"\n",
    "\n",
    "        # 更新全局列表\n",
    "        if note_id in URL_list:\n",
    "            index = URL_list.index(note_id)\n",
    "            commentNr_list[index] = comments\n",
    "            datePublished_list[index] = date_published\n",
    "            starNr_list[index] = stars\n",
    "            authorCollectNr_list[index] = collect_nr\n",
    "            authorFansNr_list[index] = fans_nr\n",
    "            authorNoteNr_list[index] = note_nr\n",
    "            video_urls[index] = video_url\n",
    "            caption_list[index] = caption  # 存储提取的帖子内容\n",
    "            print(f\"已提取附加字段，笔记ID: {note_id}\")\n",
    "        else:\n",
    "            print(f\"笔记ID {note_id} 未在URL_list中找到。\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"提取附加字段时出错，笔记ID: {note_id}, 错误: {str(e)}\")\n",
    "\n",
    "def ensure_search_results():\n",
    "    \"\"\"\n",
    "    确保已导航到搜索结果页面，并选择“图文”模式。\n",
    "    \"\"\"\n",
    "    current_url = browser.current_url\n",
    "    print(f\"当前URL: {current_url}\")\n",
    "\n",
    "    search_url = f'https://www.xiaohongshu.com/search_result?keyword={key_word}&source=web_explore_feed'\n",
    "    browser.get(search_url)\n",
    "\n",
    "    try:\n",
    "        # 等待页面标题包含关键词\n",
    "        wait = WebDriverWait(browser, 15)\n",
    "        wait.until(EC.title_contains(key_word))\n",
    "        print(\"已导航到搜索结果页面。\")\n",
    "    except:\n",
    "        print(\"导航到搜索结果页面时超时。\")\n",
    "        browser.quit()\n",
    "        exit()\n",
    "\n",
    "    try:\n",
    "        selectors = [\n",
    "            \"//div[text()='图文']\",\n",
    "            \"//div[contains(@class, 'tab')]//span[text()='图文']\",\n",
    "            \"//div[contains(@class, 'filter')]//div[text()='图文']\",\n",
    "            \"//*[contains(text(), '图文')]\"\n",
    "        ]\n",
    "\n",
    "        for selector in selectors:\n",
    "            try:\n",
    "                element = WebDriverWait(browser, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, selector))\n",
    "                )\n",
    "                print(f\"找到元素，选择器: {selector}\")\n",
    "                element.click()\n",
    "                time.sleep(2)  # 等待模式切换\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            print(\"未找到“图文”标签，可能页面结构已更改。\")\n",
    "    except Exception as e:\n",
    "        print(f\"切换视图时出错: {e}\")\n",
    "\n",
    "    # 等待任何内容加载完成\n",
    "    time.sleep(3)\n",
    "\n",
    "def download_image(url, save_path):\n",
    "    \"\"\"\n",
    "    下载图片并保存到指定路径。\n",
    "    \n",
    "    Args:\n",
    "        url (str): 图片的URL地址。\n",
    "        save_path (str): 图片保存的本地路径。\n",
    "    \n",
    "    Returns:\n",
    "        bool: 下载是否成功。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # 检查请求是否成功\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"下载图片时出错，URL: {url}, 错误: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# 创建目录用于存储图片\n",
    "def create_directories():\n",
    "    \"\"\"\n",
    "    创建用于存储主图片和头像图片的目录。\n",
    "    \"\"\"\n",
    "    if not os.path.exists('images'):\n",
    "        os.makedirs('images')\n",
    "    if not os.path.exists('avatars'):\n",
    "        os.makedirs('avatars')\n",
    "\n",
    "# 调用目录创建函数\n",
    "create_directories()\n",
    "\n",
    "# 定义进度条:实时跟踪已爬取的笔记数量\n",
    "qbar = tqdm(total=num, desc=\"爬取进度\")\n",
    "\n",
    "ensure_search_results()\n",
    "\n",
    "while len(URL_list) < num:\n",
    "    for _ in range(3):\n",
    "        browser.execute_script(\"window.scrollBy(0, 300);\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    parsePage(browser.page_source)\n",
    "\n",
    "    if '- THE END -' in browser.page_source or 'No more content' in browser.page_source:\n",
    "        print(f\"已到达内容末尾。总共收集: {len(URL_list)} 条\")\n",
    "        break\n",
    "\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "print(f\"总共收集的条目数: {len(URL_list)}\")\n",
    "if URL_list:\n",
    "    print(\"收集的数据样本:\")\n",
    "    for i in range(min(5, len(URL_list))):\n",
    "        print(f\"URL: {URL_list[i]}\")\n",
    "\n",
    "if len(URL_list) > num:\n",
    "    URL_list = URL_list[:num]\n",
    "    authorName_list = authorName_list[:num]\n",
    "    likeNr_list = likeNr_list[:num]\n",
    "    userURL_list = userURL_list[:num]\n",
    "    commentNr_list = commentNr_list[:num]\n",
    "    post_title_list = post_title_list[:num]\n",
    "    datePublished_list = datePublished_list[:num]\n",
    "    images_list = images_list[:num]\n",
    "    author_avatar_list = author_avatar_list[:num]  # 截断头像列表\n",
    "    starNr_list = starNr_list[:num]\n",
    "    authorCollectNr_list = authorCollectNr_list[:num]\n",
    "    authorFansNr_list = authorFansNr_list[:num]\n",
    "    authorNoteNr_list = authorNoteNr_list[:num]\n",
    "    video_urls = video_urls[:num]\n",
    "    caption_list = caption_list[:num]  # 截断帖子内容列表\n",
    "\n",
    "print(f\"截断后的总条目数: {len(URL_list)}\")\n",
    "print(\"收集的数据样本:\")\n",
    "for i in range(min(5, len(URL_list))):\n",
    "    print(f\"作者: {authorName_list[i]}, 点赞: {likeNr_list[i]}, URL: {URL_list[i]}\")\n",
    "\n",
    "qbar.close()\n",
    "\n",
    "# 提取附加字段，包括帖子内容、日期发布和评论数量\n",
    "print(\"开始提取附加字段，包括帖子内容、日期发布和评论数量...\")\n",
    "qbar = tqdm(total=len(URL_list), desc=\"已获取的笔记数量...\")\n",
    "\n",
    "for note_id, note_url in zip(URL_list, userURL_list):\n",
    "    extract_additional_fields(note_url, note_id)\n",
    "    qbar.update(1)\n",
    "    time.sleep(random.uniform(2, 4))  # 礼貌等待，避免服务器压力过大\n",
    "\n",
    "qbar.close()\n",
    "\n",
    "# 下载图片\n",
    "print(\"开始下载主图片和头像图片...\")\n",
    "\n",
    "# 下载主图片\n",
    "image_download_bar = tqdm(total=len(images_list), desc=\"下载主图片\")\n",
    "for idx, image_url in enumerate(images_list):\n",
    "    if image_url == \"N/A\":\n",
    "        image_download_bar.update(1)\n",
    "        continue\n",
    "    # 构造图片保存路径，使用note_id作为文件名\n",
    "    image_extension = os.path.splitext(image_url)[1].split('?')[0]  # 获取文件扩展名\n",
    "    if image_extension.lower() not in ['.jpg', '.jpeg', '.png', '.gif']:\n",
    "        image_extension = '.jpg'  # 默认使用.jpg\n",
    "    image_filename = f\"images/{URL_list[idx]}{image_extension}\"\n",
    "    success = download_image(image_url, image_filename)\n",
    "    if not success:\n",
    "        image_filename = \"N/A\"  # 如果下载失败，标记为N/A\n",
    "    images_list[idx] = image_filename  # 更新为本地路径\n",
    "    image_download_bar.update(1)\n",
    "image_download_bar.close()\n",
    "\n",
    "# 下载头像图片\n",
    "avatar_download_bar = tqdm(total=len(author_avatar_list), desc=\"下载头像图片\")\n",
    "for idx, avatar_url in enumerate(author_avatar_list):\n",
    "    if avatar_url == \"N/A\":\n",
    "        avatar_download_bar.update(1)\n",
    "        continue\n",
    "    # 构造头像保存路径，使用note_id作为文件名\n",
    "    avatar_extension = os.path.splitext(avatar_url)[1].split('?')[0]  # 获取文件扩展名\n",
    "    if avatar_extension.lower() not in ['.jpg', '.jpeg', '.png', '.gif']:\n",
    "        avatar_extension = '.jpg'  # 默认使用.jpg\n",
    "    avatar_filename = f\"avatars/{URL_list[idx]}{avatar_extension}\"\n",
    "    success = download_image(avatar_url, avatar_filename)\n",
    "    if not success:\n",
    "        avatar_filename = \"N/A\"  # 如果下载失败，标记为N/A\n",
    "    author_avatar_list[idx] = avatar_filename  # 更新为本地路径\n",
    "    avatar_download_bar.update(1)\n",
    "avatar_download_bar.close()\n",
    "\n",
    "print(\"所有图片下载完成。\")\n",
    "\n",
    "# 创建数据字典，包括“Author Avatar”和“Caption”\n",
    "data = {\n",
    "    'Author Name': authorName_list,\n",
    "    'Likes': likeNr_list,\n",
    "    'Comments': commentNr_list,\n",
    "    'Post Title': post_title_list, \n",
    "    'Caption': caption_list,  # 包含帖子内容\n",
    "    'Date Published': datePublished_list,\n",
    "    'Images': images_list,  # 仅主图的本地路径\n",
    "    'Author Avatar': author_avatar_list,  # 单独的头像列表的本地路径\n",
    "    'Stars': starNr_list,\n",
    "    'Author Collect Nr': authorCollectNr_list,\n",
    "    'Author Fans Nr': authorFansNr_list,\n",
    "    'Author Note Nr': authorNoteNr_list,\n",
    "    'Video URL': video_urls,\n",
    "    'URL': URL_list,\n",
    "    'User URL': userURL_list\n",
    "}\n",
    "\n",
    "# 创建DataFrame并保存为CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index('URL', inplace=True)\n",
    "print(df.head())\n",
    "df.to_csv('scraped_data2.csv', encoding='utf-8-sig')\n",
    "print(\"数据已保存到 'scraped_data2.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xiaoliu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
